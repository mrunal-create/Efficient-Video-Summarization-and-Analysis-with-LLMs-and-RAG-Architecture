{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdVfgUs2TUmR"
      },
      "outputs": [],
      "source": [
        "!pip install moviepy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube ffmpeg-python --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWWO1qihYzOD",
        "outputId": "9793239f-5500-434a-8d38-6fbe15ceed57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m51.2/57.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition --quiet\n",
        "!pip install pydub --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8cxJ9WLZdzB",
        "outputId": "26fa83f4-0536-4ad7-e0fc-afefee9482b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install Office365-REST-Python-Client"
      ],
      "metadata": {
        "id": "ZejtboJiksa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install requests_ntlm"
      ],
      "metadata": {
        "id": "zvLmJ-jso5XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall ffmpeg ffmpeg-python"
      ],
      "metadata": {
        "id": "BdoegJSmawoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "import ffmpeg\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "BfEHEnZYgdDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "from pydub.utils import make_chunks\n",
        "import os"
      ],
      "metadata": {
        "id": "Pdx_x4CZZpVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyirQH1PVpeF",
        "outputId": "9280e050-c76b-456e-edb5-3d2f300cd078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_mp4_playable(file_path):\n",
        "  try:\n",
        "      # Probe the file to get information about it\n",
        "      probe = ffmpeg.probe(file_path)\n",
        "      # Check if the file format is recognized as video\n",
        "      if 'streams' in probe and any(stream['codec_type'] == 'video' for stream in probe['streams']):\n",
        "          print(\"MP4 file is playable.\")\n",
        "          return True\n",
        "      else:\n",
        "          print(\"MP4 file is corrupt or non-playable.\")\n",
        "          return False\n",
        "  except ffmpeg.Error as e:\n",
        "      #print(\"Error occurred:\", e.stderr)\n",
        "      return False"
      ],
      "metadata": {
        "id": "PeH9-VU-43Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio(link, output_file_path):\n",
        "\n",
        "  if urlparse(link).netloc == \"www.youtube.com\":\n",
        "    yt = YouTube(link)\n",
        "    video_path = yt.streams[0].url\n",
        "\n",
        "  elif urlparse(link).netloc == \"www.linkedin.com\":\n",
        "    r = requests.get(link)\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    data_linkedin = json.loads(soup.find('script', type='application/ld+json').text)\n",
        "    if data_linkedin['isAccessibleForFree'] == True:\n",
        "      video_path = json.loads(soup.video['data-sources'])[0]['src']\n",
        "    else:\n",
        "      print(\"Sorry! Can't extract audio. Please make sure the video is free for access.\")\n",
        "      return\n",
        "\n",
        "  elif urlparse(link).netloc == \"www.coursera.org\":\n",
        "    r = requests.get(link)\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    data_coursera = json.loads(soup.find('script', type='application/ld+json').text)\n",
        "    video_path = data_coursera['@graph'][1]['contentURL']\n",
        "\n",
        "  elif urlparse(link).netloc not in [\"www.youtube.com\", \"www.linkedin.com\", \"www.coursera.org\"]:\n",
        "    print(\"Sorry! Can't extract audio. Please make sure the video link is valid.\")\n",
        "    return\n",
        "\n",
        "\n",
        "  else:\n",
        "    if(is_mp4_playable(link)):\n",
        "      video_path = link\n",
        "    else:\n",
        "      print(\"Sorry! Can't extract audio. Please make sure the video file exists and is not corrupted.\")\n",
        "      return\n",
        "\n",
        "\n",
        "  audio, err = (\n",
        "      ffmpeg\n",
        "      .input(video_path)\n",
        "      .output(\"pipe:\", format='mp3', acodec='libmp3lame', audio_bitrate='320k')\n",
        "      .run(capture_stdout=True)\n",
        "  )\n",
        "\n",
        "\n",
        "  with open(output_file_path, 'wb') as f:\n",
        "      f.write(audio)\n",
        "\n",
        "  print(\"Audio extraction complete.\")"
      ],
      "metadata": {
        "id": "rTYlQqzzYw2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invalid URL\n",
        "extract_audio(\"https://exampleurl\",'/content/drive/My Drive/LLM_Project/link_audio.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9hsxNAsBJO7",
        "outputId": "29ec891e-58f9-4bb7-8d5c-6f61089acd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry! Can't extract audio. Please make sure the video link is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Youtube\n",
        "extract_audio(\"https://www.youtube.com/watch?v=AM8D4j9KoaU&pp=ygUVZGV0ciBvYmplY3QgZGV0ZWN0aW9u\",'/content/sample_data/youtube_audio.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7puCHCx61pLZ",
        "outputId": "c652dff0-704b-4fd1-9934-f10a44c25ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linkedin free course\n",
        "extract_audio(\"https://www.linkedin.com/learning/administrative-professional-weekly-tips/managing-morale-in-your-office?&u=74653650\",'/content/sample_data/linkedin_audio_free.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II6zsYPQyu6Z",
        "outputId": "7031d1ef-bec7-4d31-a8ad-db76c96461de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linkedin non-free course\n",
        "extract_audio(\"https://www.linkedin.com/learning/tableau-for-data-scientists/solution-connecting-to-data?resume=false&u=74653650\",'/content/drive/My Drive/LLM_Project/linkedin_audio_paid.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jxas60Fz9W0",
        "outputId": "63d1bf57-b0fe-4a11-f377-88c231b44fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry! Can't extract audio. Please make sure the video is free for access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coursera free course\n",
        "extract_audio(\"https://www.coursera.org/learn/the-science-of-well-being/lecture/yaNZk/part-2-goal-setting\",'/content/sample_data/coursera_audio_free.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9KhpxIQ0s8C",
        "outputId": "e6fb1715-90a2-484c-ea35-fa7763c72e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploaded MP4 video file - correct\n",
        "extract_audio(\"/content/drive/My Drive/LLM_Project/LLM_lecture.mp4\",'content/sample_data/uploaded_MP4_audio.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd2rW5i02Ubi",
        "outputId": "1cb0bb54-20d7-4993-c4b1-d2bbf6124f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry! Can't extract audio. Please make sure the video link is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploaded MP4 video file - path doesn't exist\n",
        "extract_audio(\"/content/drive/My Drive/LLM_Project/LLM_lecture.mp3\",'/content/drive/My Drive/LLM_Project/uploaded_MP4_audio.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae4cl34T4MPO",
        "outputId": "a36fef8c-1edf-464f-bce9-6ffadddaea0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry! Can't extract audio. Please make sure the video file exists and is not corrupted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploaded MP4 video file - corrupt/non-playable file\n",
        "extract_audio(\"/content/drive/My Drive/LLM_Project/empty_video.mp4\",'/content/drive/My Drive/LLM_Project/uploaded_MP4_audio.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycEelUUl5Xt0",
        "outputId": "edcf43fc-2cac-48c0-a1b0-4801711630f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry! Can't extract audio. Please make sure the video file exists and is not corrupted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urlparse(\"https://exampleurl.uyg\").netloc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "M8jiI1gZBwmj",
        "outputId": "74d2c413-d947-4b1a-fa52-e2c4d0918b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'exampleurl.uyg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1bpeURXvsXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6lJELUZvsaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9DBHWJKvscT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqkVhmypvsfv",
        "outputId": "82dcb139-bf0a-4942-f9dc-f8d8d2d13497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aeVLi5fkvwn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_audio_chunks(audio_file, chunksize = 30000):\n",
        "    mp3_audio = AudioSegment.from_mp3(audio_file)\n",
        "\n",
        "    # Split the audio into chunks\n",
        "    chunks = make_chunks(mp3_audio, chunksize)\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "mRkKiJmbwDo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_chunks(audio_chunks):\n",
        "  whole_speech = \"\"\n",
        "  recognizer = sr.Recognizer()\n",
        "  for i, chunk in enumerate(audio_chunks):\n",
        "    audio = chunk.export(format=\"wav\")\n",
        "    with sr.AudioFile(audio) as source:\n",
        "      audio_data = recognizer.record(source)\n",
        "      try:\n",
        "        text = recognizer.recognize_google(audio_data)\n",
        "        print(f\"Chunk {i+1}: {text}\")\n",
        "        whole_speech += text\n",
        "      except sr.UnknownValueError:\n",
        "        print(f\"Chunk {i+1}, Google Speech Recognition could not understand audio\")\n",
        "      except sr.RequestError as e:\n",
        "        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
        "  return whole_speech"
      ],
      "metadata": {
        "id": "QTloXPr3wEMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sound_file = \"/content/sample_data/youtube_audio.mp3\""
      ],
      "metadata": {
        "id": "dbWOx4kNwTcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_chunks = divide_audio_chunks(sound_file)"
      ],
      "metadata": {
        "id": "CsxaEeX1wWxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech = process_chunks(audio_chunks)\n",
        "print(\"\\n\\n Entire Speech\")\n",
        "print(speech)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVFd1kIOwceg",
        "outputId": "32fc3443-6fcf-42c1-daa7-7d3c1f0ff44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: I need to learn Transformers I have this kind of imposter syndrome crisis recently don't get me wrong I feel pretty comfortable working with almost any computer vision algorithm that is popular right now but when it comes to Transformers I don't feel comfortable enough that's why I decided that this week will dive deeper in the world of object detection Transformers and we will learn how to train one of them on\n",
            "Chunk 2: play some data set so if you are just like me and you would like to learn something new today sit back relax make some coffee and prepare yourself for 25 minutes of pure struggle but I promise that we will use plenty of super cool and useful tools today like by torch lightning Transformers supervision tensorboard so there's a lot of unpack let's not waste any more time and train some models and as usual I prepared for you a\n",
            "Chunk 3: locate The jupyter Notebook you will find it in the description below so you can follow my steps during the tutorial we scroll through the overview and table of content I highly encourage you guys to read it you can learn a lot about the detr model and we stop by and video semi command is the first cell that we are going to execute it is here to confirm that we have access to the GPU and the execution may take a little bit of time because we first need to connect\n",
            "Chunk 4: the actual GPU on the server but when it's done it prints out the architecture of the GPU that we have at our disposal and an error case it's Tesla T4 now if you don't see the same output in that cell that most likely means that your run time is not GPU accelerated and in that case you need to navigate to runtime change run time type and select GPU from the drop-down I guess that value is known by default so you just need to switch it up to GPU and you should be ready to go\n",
            "Chunk 5: same time keep in mind that that pop up might look a little bit different than what you see on the screen that's because I'm using Google call up Pro and you have less option in the free version cool next thing is creating home constant I like to have it because it helps me out to manage paths to files and directories and when we examine its value in case of Google call up it leads to slash that content directory and this is exactly the same directory that is the root of your file viewer\n",
            "Chunk 6: turn the left top of Google call up we can confirm that by running LS command and we see that we get exactly the same result in the file viewer and in the terminal I know it might sound super intuitive for many of you but I usually get like 10 comments under each video asking about path management so I just wanted to make sure that everybody understands where we are cool now let's set up our python environment we have few dependencies that we need to install will you supervision to annotate our frames and manage our\n",
            "Chunk 7: directions will use Transformers obviously to load our DTR model will you steam to load the backbone for that model rubber floor to download the data set and pytorch lightning to manage the training at this point we are basically ready to start the training but before we do that I always like to use the pre-trained model and run it on some example image just to\n",
            "Chunk 8: turn that all the installation steps were done properly so let's do that we start by downloading our example image that one is coming from my private Gallery will see it in a second and after downloading is completed we should be able to see that image in our file manager on the left hand side we can see that the name of the image is dog jpeg so let's use that name and our home constant to create the path to that image in just a second will be able to use\n",
            "Chunk 9: that path to load damage and run it for the network but before we do that we need to load the actual Network into the memory with Transformers it's actually pretty easy all we need to do is load two classes from Transformers package the first one is DTR for object detection and the second one is DTR image processor the first one is the actual model the second one is the set of utilities that will use in conjunction with that model\n",
            "Chunk 10: for example pre-process the image or post-process the detections we want to be able to run those models on CPU or on GPU depending on the hardware that we have at our disposal and because we are using pytorch this is actually pretty easy using torch Cuda is available we can set the value of the device constant to CPU or to Cuda 0 if the GPU is accessible we will also create another\n",
            "Chunk 11: instant call checkpoint that will Define the version of the base model that will use for our training will also sprinkle a few more constants that will use for detection post-processing later on but at this point let's load the model the whole process may take a few seconds to complete but when it's done the whole model architecture will get printed on the screen now we can scroll a bit lower to the part where we will actually run the in France and let me quickly go through the coat\n",
            "Chunk 12: and that will use we start with George no garage to disable grading calculation which will result in reduced memory consumption of our model next part is pretty straightforward we use opencv to read the image from the hard drive and use image processor to apply all necessary Transformations before the inference at the very end of the line we use two devices to make sure that image python tensor is leaving on the same\n",
            "Chunk 13: advice as model weights now that we have our inputs ready we can push them through the model and save the results in the outputs variable that output is still raw so to make it useful we need to process it using the same image processor as before it will take care of bounding box scaling and filtering by confidence among other things now we are ready to visualize our predictions and to do it will utilize supervision package we start by conver\n",
            "Chunk 14: I think our results to supervision detections and then we create bounding box annotator that we can use to annotate the frames the last thing that we need to do is call annotate method to pass our image detections as well as the labels that we generated one line above to annotate our image with class names and confidences now we can run the cell and wait a few seconds for the model to generate the predictions we can see that the model generated double\n",
            "Chunk 15: so for example in the case of backpack or the person we have two bounding boxes instead of one it's a bit hard to see but the car on the right side of the image also got detected two times but this time we have two bounding boxes with two different classes car and truck double detection is very common problem with object detection models like yellow for example and it can be easily solved with non Max suppression algorithm when we scroll a little bit lower we can see another coat snip\n",
            "Chunk 16: this one is basically identical to the previous one the only difference is that we apply non maxion on our detections we can see that the line that previously only converted results into the Texans right now uses nms now when we run the model we notice two things first of all it produces detections much faster because it's already hot and all the ways are loaded into the memory second of all thanks to that small nms change we get only\n",
            "Chunk 17: books for object so far so good our model in first that's a very good sign looks like we installed everything properly and now finally we can move forward and shift our Focus to the actual training and obviously if you want to train modeling custom data you first need to have your data set incorrect format for this particular video I expect you to have your daytime calculator in the top right\n",
            "Chunk 18: turn on in the description below and shout out to Jacob for great explanation and in the meantime let me show you the data set that I'm going to use in today's tutorial so I go to roblox.com sign in and after my workspace is loaded I select football player detection and that's the data set we are going to use to train our custom model it comes from recent Google Bundesliga competition and allows me to detect\n",
            "Chunk 19: the players goalkeepers referees and bull if you want to follow along with this data set the only thing that you need is RoboCop so let's grab it we navigate back to Roblox and settings then rubber floor API and copy our key then we navigate back to notebook paste the key heat shift enter two times and our data set is being downloaded that process may take a little bit of time because we have\n",
            "Chunk 20: I like to 100 images to download so in the meantime let me show you how you would proceed if you wouldn't have that snippet already in the coop and you would need to obtain it on your own I go back to my project first select the version I have four to choose from I keep the latest one and then click export button select Coco Jason as the format click continue and after a few seconds I should have a cold snippet that I can just copy and paste into notebook in the\n",
            "Chunk 21: what time our data set to download it and we can see it and file explorer on the left side so in the data set directory we have football player detection version 1 and importantly we have free subdirectories train validate and test that will use for training and validation of our data set validation of our model if we want to find our data set we can use location property of data set class that stores that have\n",
            "Chunk 22: to Route the directory of the holidays having annotation files is one thing but being able to use them during training is another fortunately pytorch comes with very handy Coco detection class that will right now utilize to build our data sets and later the data loaders we start by extending that cocoa detection class and that's because we have few custom things that we need to add let's start with Constructor first of all peyote Coco detection\n",
            "Chunk 23: expect to pass the first one to image directory the second one to annotation Jason in our case images and annotations are located in the same directory and that means that we can easily infer the path to annotations by just joining the name of The annotation file with the path to the images directory and then pass those two as two separate arguments to pytorch Cocoa detection parent class now another thing that we need to remember about is that we need to pre-process our images before we\n",
            "Chunk 24: can you put them for neural network and that's why we inject image processor in the contractor save it as class field to then use it in get item method depending on the object detection Transformer that you would like to train we would inject different image processor in the contractor for Cocoa detection and that's because usually in Transformer API every model has dedicated image processor that you should use like I said we divide that our data in\n",
            "Chunk 25: Parts test train invalidate and right now we need to load them a separate cocoa data sets when we run the cell we can see how many images we have in each individual substance if we go back to our River flow dashboard we can see that those numbers correspond to the distribution that we defined in our project now I guess it would be good idea to verify if our Coco annotations are being read correctly to do that we scroll a little bit lower to next cold snippet here is quite straightforward\n",
            "Chunk 26: we start by using python API to get IDs of all images that we have in our data set then we randomly select one of them and load it from the hard drive at the same time without annotations corresponding to the same image and then once again you supervision this time to convert cocoa annotations into detections object that we can once again use to annotate bounding boxes on the source image I know it's a lot of talking but let's take a look at the results\n",
            "Chunk 27: Boise hours first image being annotated with bounding boxes everything seems to be okay we have the right classes and the bounding boxes are in the right place because we select image at random we can just rerun the cell and take a look at different image from the data set to build data loader we need to Define few things the data set the patch size and the correct function we already spoke a lot about the data sets so let's focus on the two other arguments to speed up the training the neural network can\n",
            "Chunk 28: who's really consumer multiple images at the same time and that group of images is called batch the larger the batch size the faster we can consume data and learn however there is a catch as the batch size gross so is the memory consumption of our neural network and that essentially means that our training breaks because we simply don't have enough memory to start our model weights our images our gradients at the same time all in all selecting a batch\n",
            "Chunk 29: what is simply a balance between the speed of the training and the memory allocation at the same time our new network expect to get the patch wrapped in some specific data structure and that's where collect function comes in it is responsible for putting everything together in such a way that it will be understandable for our new network during inference and training in case of the ETR the collect function is quite specific because the outdoors of the paper decided to\n",
            "Chunk 30: use variety of image sizes during the training that means that sensors responsible for storing pixel values have different shapes and it's basically impossible to directly patch them together to solve that problem they decided to pad every image in the batch to the shape of the largest image but now you lose the information about the size of the original image well not exactly to be able to go back they decided to create additional tensor called pixel mask that\n",
            "Chunk 31: only ones and zeros one if the pixel was originally in the image and 0 if it doesn't Pretty clever solution but we need to handle that whole complexity in our data loader okay we are done with data processing and loading and now we can focus on training the model model training is essentially a gigantic Loop where every time we get the batch of data we push it forward through the neural network we calculate the loss to understand how far from desired solution we are we\n",
            "Chunk 32: use Optimizer to understand how can we get better and then go backwards for the neural network tweaking our weights so that next step will be just a bit closer to the desired solution sounds very simple but writing those training Loops can be very tedious task easy to make mistake so that's why it's pretty much a standard practice to use some library to manage the training process and in this tutorial will use titles lightning to do that so just like\n",
            "Chunk 33: put data loaders we start by extending the base class in our case lightning module if you know something about the programming you know that when it comes to inheritance there is usually a set of method that we are expected to use and we can learn about those from pytorch lightning documentation and I highly encourage you guys to read it or at least take a look at it in our case most of the stuff is pretty straightforward but there are a few tricky parts so I will take you through the whole implementation we start\n",
            "Chunk 34: extractor where we initialize our model from the same checkpoint as few minutes ago and obviously if we would train different object detection Transformer we would need to use different class to initiate it another thing that is very specific to DTR is the backbone learning rate so like I said before training is just a gigantic Loop where every iteration we take a small step towards desired solution and the size of that step is decided by learning rate usually the\n",
            "Chunk 35: neural network have the same Learning grade but not this time and we need to be able to handle that complexity in our trainer that's why our trainer takes two learning grades the first one that will be applied to the whole neural network the second one that will be applied only to the backbone we save both of those learning rates as the class Fields the next method that will require our attention is configure Optimizer and that's because learning rate is one of the key factors that influence OptumRx\n",
            "Chunk 36: the result here we need to ensure that we include not only the basic learning rate but also the Learning grade for the backbone the rest of the steps are pretty standard ones we added a little bit of logging to training and validation steps but that's about it at the end we just need to make sure that we pass train and validation data loaders and you can start training finally to keep track of the key metrics during the training will use tensorboard it will scan our logs\n",
            "Chunk 37: rectory and create real-time charge during the training okay all stars have been aligned and we can finally run our training just one more sanity check we create an instance of our python module pass one batch through it just to confirm that everything works and doesn't break just one more minute\n",
            "Chunk 38: looks like we didn't trigger some cell there it is collect function okay now now we should be able to run everything smoothly hopefully and it works cool now we have one more step to do which is actually to trigger the training I'll just adjust the amount of people and change it from\n",
            "Chunk 39: free to I don't know 50 for example and we can hit shift enter and the training starts 50 ebooks even with 600 images that's quite a lot of time so obviously we will not watch that whole process I will speed it up for you I decided to take a earlier look at the model just to make sure that the\n",
            "Chunk 40: and it doesn't do anything stupid so let's refresh the tensorboard switch to Scholars and scroll a little bit lower maybe to train lost bounding boxes it is a bit bumpy but it's still converges what about other metrics maybe training class and it's much smoother and like I said it's still converges the validation loss\n",
            "Chunk 41: does as well okay we have still 30 bucks to go so see you later the training has completed so I'd say let's put the model to the test and run some inference the code that will use to do that is basically a combination of code that we already used it can be divided into two parts the first one is picking up the random image and displaying the annotations the second one is running the model and\n",
            "Chunk 42: I'm just playing the Texans so let's execute it and take a look at the results so the top one like I said those are the annotations quite a cool image because we get to see all the classes the bottom one shows predictions and straight away we can see a lot of false positives we see multiple bounding boxes on the goalkeeper I expected that given the fact\n",
            "Chunk 43: should we got multiple bounding boxes for our test image let's change the IOU threshold something lower and try to filter them out one thing that is a bit inconvenient that we still pick the random image so we won't be able to compare that to the previous result but it is quite noticeable that we no longer see multiple bounding boxes on a goalkeeper for example something that we wouldn't be able to solve with\n",
            "Chunk 44: Mass are obviously those false detections of referees and we see that the model of the side to detect refereeing few random places around the field yeah maybe longer training would help with that let me know in the comments if you have ideas on how to solve that particular problem still I'm quite happy with this result given the fact that the actual model was developed in 2020 and since then we saw already\n",
            "Chunk 45: major Improvement inaccuracy okay let's evaluate it on the whole test set before we wrap up the whole tutorial shout out to Neils for packaging the original Coco evaluator that came together with DTR model because right now we don't need to clone the original repository we can just install it with the package let me just run the next to cells and just a few seconds we should have evaluation\n",
            "Chunk 46: how to get the overall I owe you at 50% threshold is just around 41 cool yeah that's it for today the whole video is significantly longer than I originally anticipated but I guess it's because we did dive very deep into the Python's lightning API I just hope that you\n",
            "Chunk 47: I found it useful and you learn something I can say that I learned a lot that was actually my first pytorch lightning project so I am super happy with the result if you want to see more tutorials like this make sure to like And subscribe let me know that this longer format is suitable for you thanks a lot my name is Peter and I see you next time bye\n",
            "\n",
            "\n",
            " Entire Speech\n",
            "I need to learn Transformers I have this kind of imposter syndrome crisis recently don't get me wrong I feel pretty comfortable working with almost any computer vision algorithm that is popular right now but when it comes to Transformers I don't feel comfortable enough that's why I decided that this week will dive deeper in the world of object detection Transformers and we will learn how to train one of them onplay some data set so if you are just like me and you would like to learn something new today sit back relax make some coffee and prepare yourself for 25 minutes of pure struggle but I promise that we will use plenty of super cool and useful tools today like by torch lightning Transformers supervision tensorboard so there's a lot of unpack let's not waste any more time and train some models and as usual I prepared for you alocate The jupyter Notebook you will find it in the description below so you can follow my steps during the tutorial we scroll through the overview and table of content I highly encourage you guys to read it you can learn a lot about the detr model and we stop by and video semi command is the first cell that we are going to execute it is here to confirm that we have access to the GPU and the execution may take a little bit of time because we first need to connectthe actual GPU on the server but when it's done it prints out the architecture of the GPU that we have at our disposal and an error case it's Tesla T4 now if you don't see the same output in that cell that most likely means that your run time is not GPU accelerated and in that case you need to navigate to runtime change run time type and select GPU from the drop-down I guess that value is known by default so you just need to switch it up to GPU and you should be ready to gosame time keep in mind that that pop up might look a little bit different than what you see on the screen that's because I'm using Google call up Pro and you have less option in the free version cool next thing is creating home constant I like to have it because it helps me out to manage paths to files and directories and when we examine its value in case of Google call up it leads to slash that content directory and this is exactly the same directory that is the root of your file viewerturn the left top of Google call up we can confirm that by running LS command and we see that we get exactly the same result in the file viewer and in the terminal I know it might sound super intuitive for many of you but I usually get like 10 comments under each video asking about path management so I just wanted to make sure that everybody understands where we are cool now let's set up our python environment we have few dependencies that we need to install will you supervision to annotate our frames and manage ourdirections will use Transformers obviously to load our DTR model will you steam to load the backbone for that model rubber floor to download the data set and pytorch lightning to manage the training at this point we are basically ready to start the training but before we do that I always like to use the pre-trained model and run it on some example image just toturn that all the installation steps were done properly so let's do that we start by downloading our example image that one is coming from my private Gallery will see it in a second and after downloading is completed we should be able to see that image in our file manager on the left hand side we can see that the name of the image is dog jpeg so let's use that name and our home constant to create the path to that image in just a second will be able to usethat path to load damage and run it for the network but before we do that we need to load the actual Network into the memory with Transformers it's actually pretty easy all we need to do is load two classes from Transformers package the first one is DTR for object detection and the second one is DTR image processor the first one is the actual model the second one is the set of utilities that will use in conjunction with that modelfor example pre-process the image or post-process the detections we want to be able to run those models on CPU or on GPU depending on the hardware that we have at our disposal and because we are using pytorch this is actually pretty easy using torch Cuda is available we can set the value of the device constant to CPU or to Cuda 0 if the GPU is accessible we will also create anotherinstant call checkpoint that will Define the version of the base model that will use for our training will also sprinkle a few more constants that will use for detection post-processing later on but at this point let's load the model the whole process may take a few seconds to complete but when it's done the whole model architecture will get printed on the screen now we can scroll a bit lower to the part where we will actually run the in France and let me quickly go through the coatand that will use we start with George no garage to disable grading calculation which will result in reduced memory consumption of our model next part is pretty straightforward we use opencv to read the image from the hard drive and use image processor to apply all necessary Transformations before the inference at the very end of the line we use two devices to make sure that image python tensor is leaving on the sameadvice as model weights now that we have our inputs ready we can push them through the model and save the results in the outputs variable that output is still raw so to make it useful we need to process it using the same image processor as before it will take care of bounding box scaling and filtering by confidence among other things now we are ready to visualize our predictions and to do it will utilize supervision package we start by converI think our results to supervision detections and then we create bounding box annotator that we can use to annotate the frames the last thing that we need to do is call annotate method to pass our image detections as well as the labels that we generated one line above to annotate our image with class names and confidences now we can run the cell and wait a few seconds for the model to generate the predictions we can see that the model generated doubleso for example in the case of backpack or the person we have two bounding boxes instead of one it's a bit hard to see but the car on the right side of the image also got detected two times but this time we have two bounding boxes with two different classes car and truck double detection is very common problem with object detection models like yellow for example and it can be easily solved with non Max suppression algorithm when we scroll a little bit lower we can see another coat snipthis one is basically identical to the previous one the only difference is that we apply non maxion on our detections we can see that the line that previously only converted results into the Texans right now uses nms now when we run the model we notice two things first of all it produces detections much faster because it's already hot and all the ways are loaded into the memory second of all thanks to that small nms change we get onlybooks for object so far so good our model in first that's a very good sign looks like we installed everything properly and now finally we can move forward and shift our Focus to the actual training and obviously if you want to train modeling custom data you first need to have your data set incorrect format for this particular video I expect you to have your daytime calculator in the top rightturn on in the description below and shout out to Jacob for great explanation and in the meantime let me show you the data set that I'm going to use in today's tutorial so I go to roblox.com sign in and after my workspace is loaded I select football player detection and that's the data set we are going to use to train our custom model it comes from recent Google Bundesliga competition and allows me to detectthe players goalkeepers referees and bull if you want to follow along with this data set the only thing that you need is RoboCop so let's grab it we navigate back to Roblox and settings then rubber floor API and copy our key then we navigate back to notebook paste the key heat shift enter two times and our data set is being downloaded that process may take a little bit of time because we haveI like to 100 images to download so in the meantime let me show you how you would proceed if you wouldn't have that snippet already in the coop and you would need to obtain it on your own I go back to my project first select the version I have four to choose from I keep the latest one and then click export button select Coco Jason as the format click continue and after a few seconds I should have a cold snippet that I can just copy and paste into notebook in thewhat time our data set to download it and we can see it and file explorer on the left side so in the data set directory we have football player detection version 1 and importantly we have free subdirectories train validate and test that will use for training and validation of our data set validation of our model if we want to find our data set we can use location property of data set class that stores that haveto Route the directory of the holidays having annotation files is one thing but being able to use them during training is another fortunately pytorch comes with very handy Coco detection class that will right now utilize to build our data sets and later the data loaders we start by extending that cocoa detection class and that's because we have few custom things that we need to add let's start with Constructor first of all peyote Coco detectionexpect to pass the first one to image directory the second one to annotation Jason in our case images and annotations are located in the same directory and that means that we can easily infer the path to annotations by just joining the name of The annotation file with the path to the images directory and then pass those two as two separate arguments to pytorch Cocoa detection parent class now another thing that we need to remember about is that we need to pre-process our images before wecan you put them for neural network and that's why we inject image processor in the contractor save it as class field to then use it in get item method depending on the object detection Transformer that you would like to train we would inject different image processor in the contractor for Cocoa detection and that's because usually in Transformer API every model has dedicated image processor that you should use like I said we divide that our data inParts test train invalidate and right now we need to load them a separate cocoa data sets when we run the cell we can see how many images we have in each individual substance if we go back to our River flow dashboard we can see that those numbers correspond to the distribution that we defined in our project now I guess it would be good idea to verify if our Coco annotations are being read correctly to do that we scroll a little bit lower to next cold snippet here is quite straightforwardwe start by using python API to get IDs of all images that we have in our data set then we randomly select one of them and load it from the hard drive at the same time without annotations corresponding to the same image and then once again you supervision this time to convert cocoa annotations into detections object that we can once again use to annotate bounding boxes on the source image I know it's a lot of talking but let's take a look at the resultsBoise hours first image being annotated with bounding boxes everything seems to be okay we have the right classes and the bounding boxes are in the right place because we select image at random we can just rerun the cell and take a look at different image from the data set to build data loader we need to Define few things the data set the patch size and the correct function we already spoke a lot about the data sets so let's focus on the two other arguments to speed up the training the neural network canwho's really consumer multiple images at the same time and that group of images is called batch the larger the batch size the faster we can consume data and learn however there is a catch as the batch size gross so is the memory consumption of our neural network and that essentially means that our training breaks because we simply don't have enough memory to start our model weights our images our gradients at the same time all in all selecting a batchwhat is simply a balance between the speed of the training and the memory allocation at the same time our new network expect to get the patch wrapped in some specific data structure and that's where collect function comes in it is responsible for putting everything together in such a way that it will be understandable for our new network during inference and training in case of the ETR the collect function is quite specific because the outdoors of the paper decided touse variety of image sizes during the training that means that sensors responsible for storing pixel values have different shapes and it's basically impossible to directly patch them together to solve that problem they decided to pad every image in the batch to the shape of the largest image but now you lose the information about the size of the original image well not exactly to be able to go back they decided to create additional tensor called pixel mask thatonly ones and zeros one if the pixel was originally in the image and 0 if it doesn't Pretty clever solution but we need to handle that whole complexity in our data loader okay we are done with data processing and loading and now we can focus on training the model model training is essentially a gigantic Loop where every time we get the batch of data we push it forward through the neural network we calculate the loss to understand how far from desired solution we are weuse Optimizer to understand how can we get better and then go backwards for the neural network tweaking our weights so that next step will be just a bit closer to the desired solution sounds very simple but writing those training Loops can be very tedious task easy to make mistake so that's why it's pretty much a standard practice to use some library to manage the training process and in this tutorial will use titles lightning to do that so just likeput data loaders we start by extending the base class in our case lightning module if you know something about the programming you know that when it comes to inheritance there is usually a set of method that we are expected to use and we can learn about those from pytorch lightning documentation and I highly encourage you guys to read it or at least take a look at it in our case most of the stuff is pretty straightforward but there are a few tricky parts so I will take you through the whole implementation we startextractor where we initialize our model from the same checkpoint as few minutes ago and obviously if we would train different object detection Transformer we would need to use different class to initiate it another thing that is very specific to DTR is the backbone learning rate so like I said before training is just a gigantic Loop where every iteration we take a small step towards desired solution and the size of that step is decided by learning rate usually theneural network have the same Learning grade but not this time and we need to be able to handle that complexity in our trainer that's why our trainer takes two learning grades the first one that will be applied to the whole neural network the second one that will be applied only to the backbone we save both of those learning rates as the class Fields the next method that will require our attention is configure Optimizer and that's because learning rate is one of the key factors that influence OptumRxthe result here we need to ensure that we include not only the basic learning rate but also the Learning grade for the backbone the rest of the steps are pretty standard ones we added a little bit of logging to training and validation steps but that's about it at the end we just need to make sure that we pass train and validation data loaders and you can start training finally to keep track of the key metrics during the training will use tensorboard it will scan our logsrectory and create real-time charge during the training okay all stars have been aligned and we can finally run our training just one more sanity check we create an instance of our python module pass one batch through it just to confirm that everything works and doesn't break just one more minutelooks like we didn't trigger some cell there it is collect function okay now now we should be able to run everything smoothly hopefully and it works cool now we have one more step to do which is actually to trigger the training I'll just adjust the amount of people and change it fromfree to I don't know 50 for example and we can hit shift enter and the training starts 50 ebooks even with 600 images that's quite a lot of time so obviously we will not watch that whole process I will speed it up for you I decided to take a earlier look at the model just to make sure that theand it doesn't do anything stupid so let's refresh the tensorboard switch to Scholars and scroll a little bit lower maybe to train lost bounding boxes it is a bit bumpy but it's still converges what about other metrics maybe training class and it's much smoother and like I said it's still converges the validation lossdoes as well okay we have still 30 bucks to go so see you later the training has completed so I'd say let's put the model to the test and run some inference the code that will use to do that is basically a combination of code that we already used it can be divided into two parts the first one is picking up the random image and displaying the annotations the second one is running the model andI'm just playing the Texans so let's execute it and take a look at the results so the top one like I said those are the annotations quite a cool image because we get to see all the classes the bottom one shows predictions and straight away we can see a lot of false positives we see multiple bounding boxes on the goalkeeper I expected that given the factshould we got multiple bounding boxes for our test image let's change the IOU threshold something lower and try to filter them out one thing that is a bit inconvenient that we still pick the random image so we won't be able to compare that to the previous result but it is quite noticeable that we no longer see multiple bounding boxes on a goalkeeper for example something that we wouldn't be able to solve withMass are obviously those false detections of referees and we see that the model of the side to detect refereeing few random places around the field yeah maybe longer training would help with that let me know in the comments if you have ideas on how to solve that particular problem still I'm quite happy with this result given the fact that the actual model was developed in 2020 and since then we saw alreadymajor Improvement inaccuracy okay let's evaluate it on the whole test set before we wrap up the whole tutorial shout out to Neils for packaging the original Coco evaluator that came together with DTR model because right now we don't need to clone the original repository we can just install it with the package let me just run the next to cells and just a few seconds we should have evaluationhow to get the overall I owe you at 50% threshold is just around 41 cool yeah that's it for today the whole video is significantly longer than I originally anticipated but I guess it's because we did dive very deep into the Python's lightning API I just hope that youI found it useful and you learn something I can say that I learned a lot that was actually my first pytorch lightning project so I am super happy with the result if you want to see more tutorials like this make sure to like And subscribe let me know that this longer format is suitable for you thanks a lot my name is Peter and I see you next time bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sound_file = \"/content/sample_data/linkedin_audio_free.mp3\""
      ],
      "metadata": {
        "id": "bMuFBJl20uYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_chunks = divide_audio_chunks(sound_file)\n",
        "speech = process_chunks(audio_chunks)\n",
        "print(\"\\n\\n Entire Speech\")\n",
        "print(speech)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHwBlPEA1FrL",
        "outputId": "b8fe4261-9173-46bc-834c-ca899bc12d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: how is the morale in your office is everyone glad to work there what is your contribution to the energy that exists in your work environment I hope that you are one of the main reasons people love coming to work as a seasoned administrative support professional I have worked in a number of different offices over my 30-year career one of my defining moments was when I was in a volatile work environment\n",
            "Chunk 2: however instead of getting sucked into it I realized it was my decision how I manage my morale and what I contributed I had just as much influence as others and setting a tone for the office and I chose to make it better not worse morale and our offices is mainly seen as the attitudes Outlook and satisfaction of the employees who work there I have found that of\n",
            "Chunk 3: times were the morale is low or high is attributed to the leadership in that company it is very easy to point fingers outward and upward for morale is low however I don't subscribe to that philosophy I understand think it is the responsibility of everyone who works in that space to help keep the energy positive and promote a culture of optimism regardless of the circumstances\n",
            "Chunk 4: one finds oneself in although it is much easier to delegate that responsibility to someone else it is definitely not productive to do so Heroes I increase morale in my work environment the first area that is the most effective is my personal attitude I try to ensure my words are not negative my body language is welcoming and I try to be a friendly smile for all that I come in contact with\n",
            "Chunk 5: I often ask people how they are doing and actually take the time to listen to their answer I maintain good eye contact and take a genuine interest in their well-being next I tried to encourage informal lunches with those in my immediate work group these will either be potluck where everyone brings in the dish to share or we just get together away from our desk and eat our lunches as a group we don't spend time discussing\n",
            "Chunk 6: work for rather talking about our interest our families and our plans for the upcoming weekend it's a time for everyone to get to know one another better and see each other in a different light I learned so much about my co-workers when we do this lastly I try to recognize fellow employees and even those I provided administrative support when they do a good job it's important that people's accomplishments are recognized and that they are not taking\n",
            "Chunk 7: I know this is something I personally appreciate so I try to pass it on whenever possible office morale is something that doesn't need to be a problem quite the contrary we as administrative support professionals can be a driving force to ensure our office environments are productive warm and inviting for all\n",
            "\n",
            "\n",
            " Entire Speech\n",
            "how is the morale in your office is everyone glad to work there what is your contribution to the energy that exists in your work environment I hope that you are one of the main reasons people love coming to work as a seasoned administrative support professional I have worked in a number of different offices over my 30-year career one of my defining moments was when I was in a volatile work environmenthowever instead of getting sucked into it I realized it was my decision how I manage my morale and what I contributed I had just as much influence as others and setting a tone for the office and I chose to make it better not worse morale and our offices is mainly seen as the attitudes Outlook and satisfaction of the employees who work there I have found that oftimes were the morale is low or high is attributed to the leadership in that company it is very easy to point fingers outward and upward for morale is low however I don't subscribe to that philosophy I understand think it is the responsibility of everyone who works in that space to help keep the energy positive and promote a culture of optimism regardless of the circumstancesone finds oneself in although it is much easier to delegate that responsibility to someone else it is definitely not productive to do so Heroes I increase morale in my work environment the first area that is the most effective is my personal attitude I try to ensure my words are not negative my body language is welcoming and I try to be a friendly smile for all that I come in contact withI often ask people how they are doing and actually take the time to listen to their answer I maintain good eye contact and take a genuine interest in their well-being next I tried to encourage informal lunches with those in my immediate work group these will either be potluck where everyone brings in the dish to share or we just get together away from our desk and eat our lunches as a group we don't spend time discussingwork for rather talking about our interest our families and our plans for the upcoming weekend it's a time for everyone to get to know one another better and see each other in a different light I learned so much about my co-workers when we do this lastly I try to recognize fellow employees and even those I provided administrative support when they do a good job it's important that people's accomplishments are recognized and that they are not takingI know this is something I personally appreciate so I try to pass it on whenever possible office morale is something that doesn't need to be a problem quite the contrary we as administrative support professionals can be a driving force to ensure our office environments are productive warm and inviting for all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sound_file = \"/content/sample_data/coursera_audio_free.mp3\""
      ],
      "metadata": {
        "id": "QUr2xAbpbz0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_chunks = divide_audio_chunks(sound_file)\n",
        "speech = process_chunks(audio_chunks)\n",
        "print(\"\\n\\n Entire Speech\")\n",
        "print(speech)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohPB0O4hb3TL",
        "outputId": "0a8ef5d7-0a09-47a1-ae7f-a041861a4797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: the last thing will end on is one of the most empirically validated sets of things for Behavior change and that's thinking about your goals in a very specific way setting your goals using a couple techniques that can make your goal achievement a lot easier so we all kind of think we have goals like some of you may have goals for some of the stuff you mentioned but to actually actualize those you have to consider them as goals research suggests that thinking about your goals and very specific ways can let you achieve them even more\n",
            "Chunk 2: and so a feel of the ways that you can think about them first start with the idea of figuring out what your goal is and so that is what I'm going to refer to as goal specificity and I mean something very specific haha about goal specificity it's this idea that you have some quantitative Precision about your goal like yes I'm going to meditate but how like where like am I going to meditate here in my house am I going to meditate by myself am I going to meditate today at 8:00 p.m. how many times a week am I going to meditate for how long\n",
            "Chunk 3: like the quantitative specificity with which you define your goal it turns out really seems to matter and it matters in the way that you kind of get specific about the tasks and the way you're going to do it that specificity seems to give you a plan of how to enact it and so we can see that across a bunch of different studies I'll just kind of give you one these folks had had participants doing a hand-eye coordination task and they wanted to set a goal of doing really well at it and so they just asked subjects if they had a specific performance goal\n",
            "Chunk 4: do you have a specific goal do you want to like either a percentage correct that you want to hit or a specific speed like what your specific goal some subjects had it some subjects didn't but if you look at later subjects performance what you find is those who have more specific goals had a better task strategy so when you have this really specific goal it kind of forces you to figure out how you're going to do it and it's actually figuring out how you're going to do it that seems to lead to better performance and so the upshot is for whatever goals you're thinking about make those goals\n",
            "Chunk 5: incredibly specific like the who what where when all those different parts write them down see it quantitatively and that will help you that's part number one knowing what the goal is part number two though is thinking about your goal and what it's actually going to give you and kind of what are the sort of problems that might lead you not to do it and so for a long time folks have thought in terms of like positive thinking like if you just think about your goals it's going to like be awesome there's a lot of like cheesy Stuff if you look at positive thinking on the internet it\n",
            "Chunk 6: it's actually really helpful to think about the outcome in a lot of detail kind of indulge and think about those outcomes but to get the positive benefit from doing that you also have to do something else which is been the same amount of time as you do thinking about your goal and how awesome it would be to meet it thinking about some of the obstacles that might get in your way and this is a phenomenon that researchers have referred to as mental contrast\n",
            "Chunk 7: your positive future outcomes all everything how awesome it would be once you get your goal but you follow that up by thinking about what might be the things that get in your way how does this work well imagine you're kind of indulging is positive thinking and thinking about your goal you're like oh I want to get to the top of the mountain that would be so great if I went to the top of the mountain and you do that what's the outcome well you're kind of feeling good about it but you haven't really done anything to like make that goal get closer to happening because you kind of\n",
            "Chunk 8: neglected the fact that the pain in the butt to climb up a mountain and then maybe you need to like get right the good shoes and like you haven't spent any of your kind of cognitive effort figuring out the obstacles by contrast if you only think about the obstacles you kind of dwell on how hard it is then you're never going to get around to doing anything cuz you're going to be such a pain to get the shoes and so on like without the kind of thinking about how good the positive is going to be you don't get both however if you actually take the time to again intentionally and effort fully do both first indulge in how great it would be and then\n",
            "Chunk 9: what do you think about what the obstacles are it turns out that you now have visualized both things that you need to succeed and this is what mental contrasting is and there's a bunch of work suggesting that this technique seems to work in a variety of different contexts here's just one again kind of back to our healthy eating goals and he's researchers actually brought women in who had the goal of eating more fruits and vegetables and they taught them this mental contrasting technique they said okay imagine like what the great outcome will be if you actually eat your food is vegetables people visualize\n",
            "Chunk 10: I have to bring them with me or like I have to avoid the cookie or whatever but now they have both pieces and they can kind of see that they have to overcome these obstacles in order to get to that good thing and so the question is how does that work does it actually affect their healthy eating and so here's just a plot of how many fruits and vegetables these people are eating across the week and across time from\n",
            "Chunk 11: Baseline all the way over to 24 months later this is like a 2-year follow-up of these folks and what you find these circles are the mental contrasting case is that in the beginning just kind of thinking about or indulging is good enough but over time if you really want to keep overcoming your obstacles it seems like having both of those things in place helps and it seems to help even way down the line this is like 24 weeks out 24 months out excuse me that people are doing this stuff that this mental contrasting is still having this interesting effect so it seems like it works pretty well and so\n",
            "Chunk 12: this is the idea of goal visualization don't just visualize the good part take time to do both parts but both of these aren't enough unless you add in the third component to which I'm going to call Goal planning and we all know this like we could have had a really specific goal when I go to the dining hall I'm going to eat the like healthy apple right and we could have like you know about the outcome but sometimes you pop yourself into the situation when you're supposed to do the thing and there's this tempting thing\n",
            "Chunk 13: maybe you thought about the obstacle but when you're in the situation in the here and now now that situation is affecting you implicitly now it's like automatically like grab the pizza grab the pizza so how do you intervene on the automatic wave situations affect you without realizing it like how do you intervene on the fact that the candy jar just being present is going to mess you up and this is what a researcher Peter go with her has figured out he's figured out a way to not just have a plan but to have a plan so implicitly that it allows you to\n",
            "Chunk 14: to get through situations like this automatically without having to recruit much willpower and it's a set of techniques that he refers to as implementation intentions basically the idea is that it's a strategy that you have in the form of a if then plan that can help you lead to better goal attainment but it works in some of the same ways as a mental contrasting where you just kind of practice it outside the situation like you just have time will you think and you visualize if I'm in the dining hall and I see the pizza I will turn\n",
            "Chunk 15: Play Walk Away grab the orange like if I'm in the dining hall and I see the pizza I will turn away and grab the orange you set it up in a really it sounds really silly but you set it up in a very specific if then plan and it turns out that are automatic systems can pay attention to that stuff like our automatic systems listen if you put it in with that level of specificity and it seems to actually increase habit change and performance one of the most powerful ways goats are talks about the implementation intentions is to help you remember stuff it's also a very powerful intervening\n",
            "Chunk 16: I think of some specific action I'm going to do like when I grabbed the door knob to leave think Keys when I grabbed the door knob to leave think keys and like automatically as I grab the door knob all of a sudden that plan pops up and I can actually help me again it sounds like why would visualizing this if then plan help but it turns out empirically it just seems to work it seems to kind of act on our automatic urges and our automatic motor actions and so here's just one piece of evidence for this go with certain colleagues\n",
            "Chunk 17: I tried to look at whether or not people having implementation intentions this really specific if then plan could help them kind of do well on their New Year's resolutions and I kind of particular form of new resolutions your resolutions that you have after you get back from break so some of you may have experienced this you're going home for Christmas break and you're thinking about next semester and you're like next semester I'm going to have this goal some of those goals might be easy like some of them might actually be hard real things you want to change my question is what controls whether or not you\n",
            "Chunk 18: whether they were easier hard and they asked like do you have this implementation attention like have you really practiced like when I get back if this happens then I will do that and you just saw a naturally who had this sort of thing and then he looked back and tried to see whether or not people succeeded and here's what he finds if you look at the percentage of people who succeed for easy to implement goals having this implementation intention form which is the yellow bar helps little but not tremendous\n",
            "Chunk 19: percentage of succeeding about threefold for Heart of goals like really having it then plan that's going to actually help you out and so that's the kind of third piece which is the goal planning really having this if then plan but now the question is like okay I'm giving you all these strategies is there some easy fast\n",
            "Chunk 20: Sonic that you can use for putting all these strategies together for any set of goals that you have from the visual from the specificity to the visualizing to the obstacles to the planning and that is a technique that researchers have come up with that they call whoop which is an acronym for wish wish outcome obstacles and plan and it's a technique developed by Gabrielle outing in at the NYU who has this wonderful book on rethinking positive thinking\n",
            "Chunk 21: method of thinking the idea is you actually do some visualization like take a few minutes a day to visualize the stuff or you think about your wish you think about what your goal is as specific as possible like I want to meditate three times this week at 8:00 p.m. on Monday Wednesday Friday for this much time etc etc and I like actually spend two minutes thinking about that then I think what's the best possible outcome of that this first\n",
            "Chunk 22, Google Speech Recognition could not understand audio\n",
            "Chunk 23: does the sort of thing work kind of having a cutesy acronym for all four of these relevant Parts well Angela Duckworth and her colleagues tested this out in school children and so they picked a set of classrooms of disadvantaged 5th graders and they taught them this technique with the acronym of like think about your wish think about the outcome all these things and then they had each of the 5th graders pick a goal that was important to them that was related to school work and he could pick their own goal but it was kind of related to that and they had them do this whoop technique every morning\n",
            "Chunk 24: play woke up for a while to kind of think about any question is what happened to their outcome in terms of their GPA their School attendance and school conduct and so on again sounds like a Cheesy technique but when you look at the outcomes it's pretty impressive so this is this is whoop but they in this paper they call it mental contrast plus implementation intentions which is basically what is is both of those things together here's what happens to the student's GPA they're actually bumping up like a hole like potential grade level from a c minus to sorry plus to a B minus so it seems like on average\n",
            "Chunk 25: play the blues singer GPA to be thinking about some academic goal they had in terms of their conduct they have better conduct rating by teachers Across Time by whooping every morning and in addition if you look at their absences you can get differences in the numbers of days that these kids are absent over time it seems like actually thinking about and doing these things over time is helping them what about an adult trying to get to their goals well again you can get people to do Studies by having them do healthy eating or exercise goals is just a little easier and so here's one on exercise goals they brought\n",
            "Chunk 26: middle-aged woman who wanted to increase their physical activity they taught them the whoop technique had them practice this what's your wish or physical activity what's your best outcome what are the obstacles what's your if then plan think about your important wish do all these specific parts and then they measured this not just a little bit of time but they actually measure their activity months and months later so this is the kind of thing that can stick over time and here's what they find in terms of physical activity women who are asked to do the whoop method versus some control where you think about\n",
            "Chunk 27: you know the physical activity is helpful for you and so on in the beginning where exercising a little bit more but then Weeks Later 4 Weeks Later 8 weeks later even 10 weeks later they're increasing their number of physical activities and minutes by about twice as much like exercising probably at least about twice a week when they hadn't really done that before so I really thinking about these things seems to be an important way to get your habits which again 10 weeks on like imagine you're you're the last resolution you made you know week one that might be great but 10 weeks on are you still doing that like this is a\n",
            "Chunk 28, Google Speech Recognition could not understand audio\n",
            "\n",
            "\n",
            " Entire Speech\n",
            "the last thing will end on is one of the most empirically validated sets of things for Behavior change and that's thinking about your goals in a very specific way setting your goals using a couple techniques that can make your goal achievement a lot easier so we all kind of think we have goals like some of you may have goals for some of the stuff you mentioned but to actually actualize those you have to consider them as goals research suggests that thinking about your goals and very specific ways can let you achieve them even moreand so a feel of the ways that you can think about them first start with the idea of figuring out what your goal is and so that is what I'm going to refer to as goal specificity and I mean something very specific haha about goal specificity it's this idea that you have some quantitative Precision about your goal like yes I'm going to meditate but how like where like am I going to meditate here in my house am I going to meditate by myself am I going to meditate today at 8:00 p.m. how many times a week am I going to meditate for how longlike the quantitative specificity with which you define your goal it turns out really seems to matter and it matters in the way that you kind of get specific about the tasks and the way you're going to do it that specificity seems to give you a plan of how to enact it and so we can see that across a bunch of different studies I'll just kind of give you one these folks had had participants doing a hand-eye coordination task and they wanted to set a goal of doing really well at it and so they just asked subjects if they had a specific performance goaldo you have a specific goal do you want to like either a percentage correct that you want to hit or a specific speed like what your specific goal some subjects had it some subjects didn't but if you look at later subjects performance what you find is those who have more specific goals had a better task strategy so when you have this really specific goal it kind of forces you to figure out how you're going to do it and it's actually figuring out how you're going to do it that seems to lead to better performance and so the upshot is for whatever goals you're thinking about make those goalsincredibly specific like the who what where when all those different parts write them down see it quantitatively and that will help you that's part number one knowing what the goal is part number two though is thinking about your goal and what it's actually going to give you and kind of what are the sort of problems that might lead you not to do it and so for a long time folks have thought in terms of like positive thinking like if you just think about your goals it's going to like be awesome there's a lot of like cheesy Stuff if you look at positive thinking on the internet itit's actually really helpful to think about the outcome in a lot of detail kind of indulge and think about those outcomes but to get the positive benefit from doing that you also have to do something else which is been the same amount of time as you do thinking about your goal and how awesome it would be to meet it thinking about some of the obstacles that might get in your way and this is a phenomenon that researchers have referred to as mental contrastyour positive future outcomes all everything how awesome it would be once you get your goal but you follow that up by thinking about what might be the things that get in your way how does this work well imagine you're kind of indulging is positive thinking and thinking about your goal you're like oh I want to get to the top of the mountain that would be so great if I went to the top of the mountain and you do that what's the outcome well you're kind of feeling good about it but you haven't really done anything to like make that goal get closer to happening because you kind ofneglected the fact that the pain in the butt to climb up a mountain and then maybe you need to like get right the good shoes and like you haven't spent any of your kind of cognitive effort figuring out the obstacles by contrast if you only think about the obstacles you kind of dwell on how hard it is then you're never going to get around to doing anything cuz you're going to be such a pain to get the shoes and so on like without the kind of thinking about how good the positive is going to be you don't get both however if you actually take the time to again intentionally and effort fully do both first indulge in how great it would be and thenwhat do you think about what the obstacles are it turns out that you now have visualized both things that you need to succeed and this is what mental contrasting is and there's a bunch of work suggesting that this technique seems to work in a variety of different contexts here's just one again kind of back to our healthy eating goals and he's researchers actually brought women in who had the goal of eating more fruits and vegetables and they taught them this mental contrasting technique they said okay imagine like what the great outcome will be if you actually eat your food is vegetables people visualizeI have to bring them with me or like I have to avoid the cookie or whatever but now they have both pieces and they can kind of see that they have to overcome these obstacles in order to get to that good thing and so the question is how does that work does it actually affect their healthy eating and so here's just a plot of how many fruits and vegetables these people are eating across the week and across time fromBaseline all the way over to 24 months later this is like a 2-year follow-up of these folks and what you find these circles are the mental contrasting case is that in the beginning just kind of thinking about or indulging is good enough but over time if you really want to keep overcoming your obstacles it seems like having both of those things in place helps and it seems to help even way down the line this is like 24 weeks out 24 months out excuse me that people are doing this stuff that this mental contrasting is still having this interesting effect so it seems like it works pretty well and sothis is the idea of goal visualization don't just visualize the good part take time to do both parts but both of these aren't enough unless you add in the third component to which I'm going to call Goal planning and we all know this like we could have had a really specific goal when I go to the dining hall I'm going to eat the like healthy apple right and we could have like you know about the outcome but sometimes you pop yourself into the situation when you're supposed to do the thing and there's this tempting thingmaybe you thought about the obstacle but when you're in the situation in the here and now now that situation is affecting you implicitly now it's like automatically like grab the pizza grab the pizza so how do you intervene on the automatic wave situations affect you without realizing it like how do you intervene on the fact that the candy jar just being present is going to mess you up and this is what a researcher Peter go with her has figured out he's figured out a way to not just have a plan but to have a plan so implicitly that it allows you toto get through situations like this automatically without having to recruit much willpower and it's a set of techniques that he refers to as implementation intentions basically the idea is that it's a strategy that you have in the form of a if then plan that can help you lead to better goal attainment but it works in some of the same ways as a mental contrasting where you just kind of practice it outside the situation like you just have time will you think and you visualize if I'm in the dining hall and I see the pizza I will turnPlay Walk Away grab the orange like if I'm in the dining hall and I see the pizza I will turn away and grab the orange you set it up in a really it sounds really silly but you set it up in a very specific if then plan and it turns out that are automatic systems can pay attention to that stuff like our automatic systems listen if you put it in with that level of specificity and it seems to actually increase habit change and performance one of the most powerful ways goats are talks about the implementation intentions is to help you remember stuff it's also a very powerful interveningI think of some specific action I'm going to do like when I grabbed the door knob to leave think Keys when I grabbed the door knob to leave think keys and like automatically as I grab the door knob all of a sudden that plan pops up and I can actually help me again it sounds like why would visualizing this if then plan help but it turns out empirically it just seems to work it seems to kind of act on our automatic urges and our automatic motor actions and so here's just one piece of evidence for this go with certain colleaguesI tried to look at whether or not people having implementation intentions this really specific if then plan could help them kind of do well on their New Year's resolutions and I kind of particular form of new resolutions your resolutions that you have after you get back from break so some of you may have experienced this you're going home for Christmas break and you're thinking about next semester and you're like next semester I'm going to have this goal some of those goals might be easy like some of them might actually be hard real things you want to change my question is what controls whether or not youwhether they were easier hard and they asked like do you have this implementation attention like have you really practiced like when I get back if this happens then I will do that and you just saw a naturally who had this sort of thing and then he looked back and tried to see whether or not people succeeded and here's what he finds if you look at the percentage of people who succeed for easy to implement goals having this implementation intention form which is the yellow bar helps little but not tremendouspercentage of succeeding about threefold for Heart of goals like really having it then plan that's going to actually help you out and so that's the kind of third piece which is the goal planning really having this if then plan but now the question is like okay I'm giving you all these strategies is there some easy fastSonic that you can use for putting all these strategies together for any set of goals that you have from the visual from the specificity to the visualizing to the obstacles to the planning and that is a technique that researchers have come up with that they call whoop which is an acronym for wish wish outcome obstacles and plan and it's a technique developed by Gabrielle outing in at the NYU who has this wonderful book on rethinking positive thinkingmethod of thinking the idea is you actually do some visualization like take a few minutes a day to visualize the stuff or you think about your wish you think about what your goal is as specific as possible like I want to meditate three times this week at 8:00 p.m. on Monday Wednesday Friday for this much time etc etc and I like actually spend two minutes thinking about that then I think what's the best possible outcome of that this firstdoes the sort of thing work kind of having a cutesy acronym for all four of these relevant Parts well Angela Duckworth and her colleagues tested this out in school children and so they picked a set of classrooms of disadvantaged 5th graders and they taught them this technique with the acronym of like think about your wish think about the outcome all these things and then they had each of the 5th graders pick a goal that was important to them that was related to school work and he could pick their own goal but it was kind of related to that and they had them do this whoop technique every morningplay woke up for a while to kind of think about any question is what happened to their outcome in terms of their GPA their School attendance and school conduct and so on again sounds like a Cheesy technique but when you look at the outcomes it's pretty impressive so this is this is whoop but they in this paper they call it mental contrast plus implementation intentions which is basically what is is both of those things together here's what happens to the student's GPA they're actually bumping up like a hole like potential grade level from a c minus to sorry plus to a B minus so it seems like on averageplay the blues singer GPA to be thinking about some academic goal they had in terms of their conduct they have better conduct rating by teachers Across Time by whooping every morning and in addition if you look at their absences you can get differences in the numbers of days that these kids are absent over time it seems like actually thinking about and doing these things over time is helping them what about an adult trying to get to their goals well again you can get people to do Studies by having them do healthy eating or exercise goals is just a little easier and so here's one on exercise goals they broughtmiddle-aged woman who wanted to increase their physical activity they taught them the whoop technique had them practice this what's your wish or physical activity what's your best outcome what are the obstacles what's your if then plan think about your important wish do all these specific parts and then they measured this not just a little bit of time but they actually measure their activity months and months later so this is the kind of thing that can stick over time and here's what they find in terms of physical activity women who are asked to do the whoop method versus some control where you think aboutyou know the physical activity is helpful for you and so on in the beginning where exercising a little bit more but then Weeks Later 4 Weeks Later 8 weeks later even 10 weeks later they're increasing their number of physical activities and minutes by about twice as much like exercising probably at least about twice a week when they hadn't really done that before so I really thinking about these things seems to be an important way to get your habits which again 10 weeks on like imagine you're you're the last resolution you made you know week one that might be great but 10 weeks on are you still doing that like this is a\n"
          ]
        }
      ]
    }
  ]
}