{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8120216,"sourceType":"datasetVersion","datasetId":4798041},{"sourceId":8127300,"sourceType":"datasetVersion","datasetId":4803305}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment setup","metadata":{"id":"EpGWUswStQEZ"}},{"cell_type":"code","source":"# !pip install -q -U transformers peft accelerate optimum\n# !pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\n# !pip install langchain\n# !pip install einops\n# !pip install optimum\n# !pip install rouge_score\n# !pip install bert_score\n# !pip install pytube ffmpeg-python --quiet\n# !pip install SpeechRecognition --quiet\n# !pip install pydub --quiet\n# !pip install moviepy --quiet\n# !pip install openai --quiet\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJ57vxf5ih2v","outputId":"92d77c3f-23d3-4de1-eba5-c388d9ffb6d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment settings","metadata":{"id":"vflg3lYz1Bws"}},{"cell_type":"code","source":"#client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"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\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import neccessary libraries","metadata":{"id":"XGf07gk3tV7b"}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport math\n# import evaluate\nimport csv\nimport re\nimport pandas as pd\nfrom sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom dotenv import load_dotenv\nfrom pytube import YouTube\nimport ffmpeg\nimport requests\nfrom urllib.parse import urlparse\nimport requests\nimport json\nfrom openai import OpenAI\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport speech_recognition as sr\nfrom pydub import AudioSegment\nfrom pydub.utils import make_chunks\nimport os\n# load_dotenv() ##load all the nevironment variables\nimport os\nimport google.generativeai as genai\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig\nfrom pytube import YouTube\nimport ffmpeg\nimport requests\nfrom urllib.parse import urlparse\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom moviepy.editor import *\nimport asyncio\nimport tarfile\nfrom zipfile import ZipFile\nfrom langchain import LLMChain, HuggingFacePipeline, PromptTemplate","metadata":{"id":"BtqOIHeHtQbY","execution":{"iopub.status.busy":"2024-04-15T14:24:11.892474Z","iopub.execute_input":"2024-04-15T14:24:11.893300Z","iopub.status.idle":"2024-04-15T14:24:17.175929Z","shell.execute_reply.started":"2024-04-15T14:24:11.893262Z","shell.execute_reply":"2024-04-15T14:24:17.175173Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{"id":"XeGe4gdg08bs"}},{"cell_type":"code","source":"#Just update model_name here and test out your model , also update id of model in code below cell 12\nconfig={\n    \"model_id\": [\"TheBloke/Llama-2-7b-Chat-GPTQ\",\n                 \"TheBloke/Llama-2-7B-AWQ\",\n                 \"TheBloke/Llama-2-7B-GGUF\",\n                 \"TheBloke/Llama-2-7B-GGML\",\n                 \"TheBloke/Llama-2-7B-fp16\",\n                 \"TheBloke/Llama-2-7B-GPTQ\",\n                 \"TheBloke/llama-2-7B-Guanaco-QLoRA-AWQ\",\n                 \"TheBloke/Llama-2-7B-AWQ\"],\n    \"hf_token\": \"...\",\n    \"model\": {\n        \"temperature\": 0.7, # [0, 0.7, .0.9, 1.1, 1.3]  Testing iteratively.\n        \"max_length\": 4000,\n        \"top_k\": 10,\n        \"num_return\": 1\n    },\n    \"dataset\": [\n        \"billsum\",\n        \"cnn_dailymail\",\n        \"big_patent\"\n    ],\n    \"eval\": ['rouge', \"bertscore\", 'meteor']\n}\n","metadata":{"id":"Ilgcn7sW06Qu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"id":"4B0V3PgStZih"}},{"cell_type":"code","source":"def call_parameter(model):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    untrainable_params = pytorch_total_params - trainable_params\n    print(f'Model {model.__class__.__name__} has {pytorch_total_params} parameters in total\\n'\\\n        f'Trainable parameters: {trainable_params}\\nUntrainable parameters: {untrainable_params}')\n    return pytorch_total_params","metadata":{"id":"mIvth5iatQeJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model architecture","metadata":{"id":"CxmLQrHytd-m"}},{"cell_type":"code","source":"def generate_model(model_id, config):\n    print(f\"Setting up model {model_id}\")\n    model = AutoModelForCausalLM.from_pretrained(model_id, use_safetensors=True,\n                            device_map='auto', trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(model_id,\n                                            device_map='auto', trust_remote_code=True)\n    return model, tokenizer","metadata":{"id":"JxZI5LT83upi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Agent:\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def __repr__(self):\n        return f'Model {self.model.__class__.__name__}'","metadata":{"id":"jCresULjzlcJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Update the model_id index as per model you want to use\nmodel, tokenizer = generate_model(config['model_id'][8], config)\nno_params = call_parameter(model)\nprint(\"=====\"*5)\nprint(f\"Model {config['model_id'][8]} has {no_params} parameters.\")","metadata":{"id":"DYmsnNkXjCRB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiments","metadata":{"id":"x506HiO4thgt"}},{"cell_type":"markdown","source":"## Implementation","metadata":{"id":"85ujx14hyZbR"}},{"cell_type":"code","source":"class Generator:\n    def __init__(self, config, agent, template):\n        self.agent = agent\n        pipeline = transformers.pipeline(\n            \"text-generation\",\n            model=self.agent.model,\n            tokenizer=self.agent.tokenizer,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n            device_map=\"auto\",\n            max_length=config['model']['max_length'],\n            do_sample=True,\n            top_k=config['model']['top_k'],\n            num_return_sequences=config['model']['num_return'],\n            pad_token_id=tokenizer.eos_token_id\n        )\n        llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': config['model']['temperature']})\n        prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n        self.llm_chain = LLMChain(prompt=prompt, llm=llm)\n\n    def generate(self, text):\n        result = self.llm_chain.invoke(text)\n        return result","metadata":{"id":"Akq4pDpW52k4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"\n              Write a summary of the following text delimited by triple backticks.\n              Return your response which covers the key points of the text.\n              ```{text}```\n              SUMMARY:\n           \"\"\"","metadata":{"id":"dQsvQqZ-tQpH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agent = Agent(model, tokenizer)\nllm_agent = Generator(config, agent, template)","metadata":{"id":"IsSok7Am0--B","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Youtube links I tested on:\n\n* \"https://www.youtube.com/watch?v=U1omz0B9FTw\"\n \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\n* https://www.youtube.com/watch?v=h5id4erwD4s\n* https://www.youtube.com/watch?v=tl30y5OOfqQ,\n* https://www.youtube.com/watch?v=6bJTEZnTT5A\n* https://www.youtube.com/watch?v=SZorAJ4I-sA\n* https://www.youtube.com/watch?v=V_2QqOEwzYU\n* https://www.youtube.com/watch?v=yw-E__nDkKU\n* https://www.youtube.com/watch?v=en2bmeB4QUo\n* https://www.youtube.com/watch?v=ch5EQgzfroo","metadata":{}},{"cell_type":"markdown","source":"# **SUMMARIZATION PART**","metadata":{}},{"cell_type":"code","source":"#final = dict()\n#intialize dictionary to store your transcript and summaries generated \n#run only once in start","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_mp4_playable(file_path):\n  try:\n      # Probe the file to get information about it\n      probe = ffmpeg.probe(file_path)\n      # Check if the file format is recognized as video\n      if 'streams' in probe and any(stream['codec_type'] == 'video' for stream in probe['streams']):\n          print(\"MP4 file is playable.\")\n          return True\n      else:\n          print(\"MP4 file is corrupt or non-playable.\")\n          return False\n  except ffmpeg.Error as e:\n      #print(\"Error occurred:\", e.stderr)\n      return False\n# Extract audio from given video\ndef extract_audio(link, output_file_path):\n  if urlparse(link).netloc == \"www.youtube.com\":\n    yt = YouTube(link)\n    video_path = yt.streams[0].url\n\n  elif urlparse(link).netloc == \"www.linkedin.com\":\n    r = requests.get(link)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    data_linkedin = json.loads(soup.find('script', type='application/ld+json').text)\n    if data_linkedin['isAccessibleForFree'] == True:\n      video_path = json.loads(soup.video['data-sources'])[0]['src']\n    else:\n      print(\"Sorry! Can't extract audio. Please make sure the video is free for access.\")\n      return\n\n  elif urlparse(link).netloc == \"www.coursera.org\":\n    r = requests.get(link)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    data_coursera = json.loads(soup.find('script', type='application/ld+json').text)\n    video_path = data_coursera['@graph'][1]['contentURL']\n\n  elif urlparse(link).netloc not in [\"www.youtube.com\", \"www.linkedin.com\", \"www.coursera.org\"]:\n    print(\"Sorry! Can't extract audio. Please make sure the video link is valid.\")\n    return\n\n\n  else:\n    if(is_mp4_playable(link)):\n      video_path = link\n    else:\n      print(\"Sorry! Can't extract audio. Please make sure the video file exists and is not corrupted.\")\n      return\n\n\n  audio, err = (\n      ffmpeg\n      .input(video_path)\n      .output(\"pipe:\", format='mp3', acodec='libmp3lame', audio_bitrate='320k')\n      .run(capture_stdout=True)\n  )\n\n\n  with open(output_file_path, 'wb') as f:\n      f.write(audio)\n\n  print(\"Audio extraction complete.\")\ndef divide_audio_chunks(audio_file, chunksize = 30000):\n    mp3_audio = AudioSegment.from_mp3(audio_file)\n    # Split the audio into chunks\n    chunks = make_chunks(mp3_audio, chunksize)\n    return chunks\n\ndef process_chunks(audio_chunks):\n  whole_speech = \"\"\n  recognizer = sr.Recognizer()\n  for i, chunk in enumerate(audio_chunks):\n    audio = chunk.export(format=\"wav\")\n    with sr.AudioFile(audio) as source:\n      audio_data = recognizer.record(source)\n      try:\n        text = recognizer.recognize_google(audio_data)\n        #print(f\"Chunk {i+1}: {text}\")\n        whole_speech += text\n      except sr.UnknownValueError:\n        print(f\"Chunk {i+1}, Google Speech Recognition could not understand audio\")\n      except sr.RequestError as e:\n        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n  return whole_speech\n## getting the transcript data from yt videos\ndef extract_audio_transcript_details(youtube_video_url):\n\n    try:\n        sound_file='audio.mp3'\n        extract_audio(youtube_video_url,sound_file)\n        #video_id=youtube_video_url.split(\"=\")[1]\n        audio_chunks = divide_audio_chunks(sound_file)\n        speech = process_chunks(audio_chunks)\n        return speech\n\n    except Exception as e:\n        raise e\n    \nyoutube_link = \"https://www.youtube.com/watch?v=U1omz0B9FTw\"\nif youtube_link:\n    video_id = youtube_link.split(\"=\")[1]\n    transcript_text=extract_audio_transcript_details(youtube_link)\n    if transcript_text:\n        generated_sample = llm_agent.generate(transcript_text)\n        print(\"SUMMARY\")\n        text=generated_sample[\"text\"].split(\"SUMMARY\")[1]\n        print(text)\n        final[transcript_text]=generated_sample[\"text\"].split(\"SUMMARY\")[1]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SUMMARY OF VIDEO**","metadata":{}},{"cell_type":"code","source":"generated_sample[\"text\"].split(\"SUMMARY\")[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('length of dictionary with summaries',len(final))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text=[]\nsummaries=[]\nfor key,value in final.items():\n    input_text.append(key)\n    summaries.append(value)  \n# Path where you want to save the CSV file which contains text and summaries\nfile_path = 'output.csv'\nwith open(file_path, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['input_text', 'summaries'])\n    # Write data rows\n    for a, b in zip(input_text, summaries):\n        writer.writerow([a, b])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EVALUATION OF GENERATED SUMMARIES USING G-EVAL ","metadata":{}},{"cell_type":"markdown","source":"Here we implement an example reference-free text evaluator using gpt-4, inspired by the G-Eval) framework which evaluates the quality of generated text using large language models. Unlike metrics like ROUGE or BERTScore that rely on comparison to reference summaries, the gpt-4 based evaluator assesses the quality of generated content based solely on the input prompt and text, without any ground truth references. This makes it applicable to new datasets and tasks where human references are sparse or unavailable.\n\nHere's an overview of this method:\n\nWe define four distinct criteria:\nRelevance: Evaluates if the summary includes only important information and excludes redundancies.\nCoherence: Assesses the logical flow and organization of the summary.\nConsistency: Checks if the summary aligns with the facts in the source document.\nFluency: Rates the grammar and readability of the summary.\nWe craft prompts for each of these criteria, taking the original document and the summary as inputs, and leveraging chain-of-thought generation and guiding the model to output a numeric score from 1-5 for each criteria.\nWe generate scores from gpt-4 with the defined prompts, comparing them across summaries.\nIn this demonstration, we're using a direct scoring function where gpt-4 generates a discrete score (1-5) for each metric. Normalizing the scores and taking a weighted sum could result in more robust, continuous scores that better reflect the quality and diversity of the summaries.","metadata":{}},{"cell_type":"code","source":"#function to clean the CSV\ndef clean_whitespace(text):\n    \"\"\"Replace more than two consecutive whitespaces with a single space.\"\"\"\n    return re.sub(r'\\s{2,}', ' ', text)\n\ndef clean_csv(input_file_path, output_file_path):\n    \"\"\"Read CSV, clean data, and write to a new CSV file.\"\"\"\n    with open(input_file_path, mode='r', newline='') as infile, \\\n         open(output_file_path, mode='w', newline='') as outfile:\n        reader = csv.reader(infile)\n        writer = csv.writer(outfile)\n        headers = next(reader)\n        writer.writerow(headers)\n        for row in reader:\n            cleaned_row = [clean_whitespace(cell) for cell in row]\n            writer.writerow(cleaned_row)\n\n\ninput_csv_path = 'output.csv'\noutput_csv_path = 'cleaned.csv'\n# Call the function to clean the CSV\nclean_csv(input_csv_path, output_csv_path)\n\n\n\n\ndf=pd.read_csv('cleaned.csv')\n# Evaluation prompt template based on G-Eval\nEVALUATION_PROMPT_TEMPLATE = \"\"\"\nYou will be given one summary written for an article. Your task is to rate the summary on one metric.\nPlease make sure you read and understand these instructions very carefully. \nPlease keep this document open while reviewing, and refer to it as needed.\n\nEvaluation Criteria:\n\n{criteria}\n\nEvaluation Steps:\n\n{steps}\n\nExample:\n\nSource Text:\n\n{document}\n\nSummary:\n\n{summary}\n\nEvaluation Form (scores ONLY):\n\n- {metric_name}\n\"\"\"\n\n# Metric 1: Relevance\n\nRELEVANCY_SCORE_CRITERIA = \"\"\"\nRelevance(1-5) - selection of important content from the source. \\\nThe summary should include only important information from the source document. \\\nAnnotators were instructed to penalize summaries which contained redundancies and excess information.\n\"\"\"\n\nRELEVANCY_SCORE_STEPS = \"\"\"\n1. Read the summary and the source document carefully.\n2. Compare the summary to the source document and identify the main points of the article.\n3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n4. Assign a relevance score from 1 to 5.\n\"\"\"\n\n# Metric 2: Coherence\n\nCOHERENCE_SCORE_CRITERIA = \"\"\"\nCoherence(1-5) - the collective quality of all sentences. \\\nWe align this dimension with the DUC quality question of structure and coherence \\\nwhereby \"the summary should be well-structured and well-organized. \\\nThe summary should not just be a heap of related information, but should build from sentence to a\\\ncoherent body of information about a topic.\"\n\"\"\"\n\nCOHERENCE_SCORE_STEPS = \"\"\"\n1. Read the article carefully and identify the main topic and key points.\n2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\nand if it presents them in a clear and logical order.\n3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n\"\"\"\n\n# Metric 3: Consistency\n\nCONSISTENCY_SCORE_CRITERIA = \"\"\"\nConsistency(1-5) - the factual alignment between the summary and the summarized source. \\\nA factually consistent summary contains only statements that are entailed by the source document. \\\nAnnotators were also asked to penalize summaries that contained hallucinated facts.\n\"\"\"\n\nCONSISTENCY_SCORE_STEPS = \"\"\"\n1. Read the article carefully and identify the main facts and details it presents.\n2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\n3. Assign a score for consistency based on the Evaluation Criteria.\n\"\"\"\n\n# Metric 4: Fluency\n\nFLUENCY_SCORE_CRITERIA = \"\"\"\nFluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n3: Good. The summary has few or no errors and is easy to read and follow.\n\"\"\"\n\nFLUENCY_SCORE_STEPS = \"\"\"\nRead the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 3.\n\"\"\"\n\n# Function to simulate evaluation score fetching\ndef get_geval_score(\n    criteria: str, steps: str, document: str, summary: str, metric_name: str\n):\n    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n        criteria=criteria,\n        steps=steps,\n        metric_name=metric_name,\n        document=document,\n        summary=summary,\n    )\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=5,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    return response.choices[0].message.content\n\n# Evaluation criteria and steps as per different metrics\nevaluation_metrics = {\n    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS)\n}\n\n# Data structure to hold the evaluation results\n#data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\nfinals=[] #list of dictionaries for each video\n# Process each summary in the DataFrame\nfor index, row in df.iterrows():\n    data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n    document = row['input_text']\n    summary = row['summaries']\n    summary_type = f\"Summary {index+1}\"  # Dynamic summary naming\n\n    # Apply each evaluation metric to the current summary\n    for eval_type, (criteria, steps) in evaluation_metrics.items():\n        result = get_geval_score(criteria, steps, document, summary, eval_type)\n        score_num = float(result.strip())  # Convert to float to handle scores like '4.5'\n        data[\"Evaluation Type\"].append(eval_type)\n        data[\"Summary Type\"].append(summary_type)\n        data[\"Score\"].append(score_num)\n    finals.append(data)    \n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten the list of dictionaries into a single DataFrame\ndef highlight_max(s):\n    is_max = s == s.max()\n    return [\n        \"background-color: lightgreen\" if v else \"background-color: white\"\n        for v in is_max\n    ]\n\nall_data = []\nfor entry in finals:\n    for eval_type, summ_type, score in zip(entry['Evaluation Type'], entry['Summary Type'], entry['Score']):\n        all_data.append({'Evaluation Type': eval_type, 'Summary Type': summ_type, 'Score': score})\n\ndf = pd.DataFrame(all_data)\n\n# Pivot the DataFrame\npivot_df = df.pivot(index='Evaluation Type', columns='Summary Type', values='Score')\n\n\npivot_df = pivot_df.round(0).astype(int)\n\n# Display the rounded DataFrame\nprint(pivot_df)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pivot_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pivot_df.to_csv('G_Eval_metrics_youtube_videos_llama_model.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Print out the mean normalized scores for each evaluation type.  The normalization will rescale the scores for each evaluation type to a range between 0 and 1, and then it will calculate the average score for the entire dataset for each evaluation type.**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('G_Eval_metrics_youtube_videos_llama_model.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T15:51:52.984236Z","iopub.execute_input":"2024-04-15T15:51:52.984626Z","iopub.status.idle":"2024-04-15T15:51:52.998569Z","shell.execute_reply.started":"2024-04-15T15:51:52.984593Z","shell.execute_reply":"2024-04-15T15:51:52.997541Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-15T15:53:35.372766Z","iopub.execute_input":"2024-04-15T15:53:35.373642Z","iopub.status.idle":"2024-04-15T15:53:35.385370Z","shell.execute_reply.started":"2024-04-15T15:53:35.373606Z","shell.execute_reply":"2024-04-15T15:53:35.384408Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"  Evaluation Type  Summary 1  Summary 10  Summary 2  Summary 3  Summary 4  \\\n0       Coherence          5           5          4          4          5   \n1     Consistency          5           5          5          5          5   \n2         Fluency          3           3          3          3          3   \n3       Relevance          5           5          4          4          5   \n\n   Summary 5  Summary 6  Summary 7  Summary 8  Summary 9  \n0          1          5          5          4          5  \n1          1          5          5          5          5  \n2          2          3          3          3          3  \n3          1          5          5          5          5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Evaluation Type</th>\n      <th>Summary 1</th>\n      <th>Summary 10</th>\n      <th>Summary 2</th>\n      <th>Summary 3</th>\n      <th>Summary 4</th>\n      <th>Summary 5</th>\n      <th>Summary 6</th>\n      <th>Summary 7</th>\n      <th>Summary 8</th>\n      <th>Summary 9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Coherence</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Consistency</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Fluency</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Relevance</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n#transpose the dataframe to have the 'Evaluation Type' as columns\ndf_transposed = df.set_index('Evaluation Type').transpose()\ndf_transposed","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:11:35.465576Z","iopub.execute_input":"2024-04-15T16:11:35.466220Z","iopub.status.idle":"2024-04-15T16:11:35.478466Z","shell.execute_reply.started":"2024-04-15T16:11:35.466182Z","shell.execute_reply":"2024-04-15T16:11:35.477422Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"Evaluation Type  Coherence  Consistency  Fluency  Relevance\nSummary 1                5            5        3          5\nSummary 10               5            5        3          5\nSummary 2                4            5        3          4\nSummary 3                4            5        3          4\nSummary 4                5            5        3          5\nSummary 5                1            1        2          1\nSummary 6                5            5        3          5\nSummary 7                5            5        3          5\nSummary 8                4            5        3          5\nSummary 9                5            5        3          5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Evaluation Type</th>\n      <th>Coherence</th>\n      <th>Consistency</th>\n      <th>Fluency</th>\n      <th>Relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Summary 1</th>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Summary 10</th>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Summary 2</th>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>Summary 3</th>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>Summary 4</th>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Summary 5</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Summary 6</th>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Summary 7</th>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Summary 8</th>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Summary 9</th>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_normalized = scaler.fit_transform(df_transposed)\ndf_normalized = pd.DataFrame(df_normalized, columns=df_transposed.columns)\ndf_normalized","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:12:33.918171Z","iopub.execute_input":"2024-04-15T16:12:33.918597Z","iopub.status.idle":"2024-04-15T16:12:33.937209Z","shell.execute_reply.started":"2024-04-15T16:12:33.918562Z","shell.execute_reply":"2024-04-15T16:12:33.936262Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Evaluation Type  Coherence  Consistency  Fluency  Relevance\n0                     1.00          1.0      1.0       1.00\n1                     1.00          1.0      1.0       1.00\n2                     0.75          1.0      1.0       0.75\n3                     0.75          1.0      1.0       0.75\n4                     1.00          1.0      1.0       1.00\n5                     0.00          0.0      0.0       0.00\n6                     1.00          1.0      1.0       1.00\n7                     1.00          1.0      1.0       1.00\n8                     0.75          1.0      1.0       1.00\n9                     1.00          1.0      1.0       1.00","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Evaluation Type</th>\n      <th>Coherence</th>\n      <th>Consistency</th>\n      <th>Fluency</th>\n      <th>Relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.75</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.75</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.75</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate the mean normalized score for each evaluation type\nmean_normalized_scores = df_normalized.mean()\nprint(mean_normalized_scores)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:12:55.568569Z","iopub.execute_input":"2024-04-15T16:12:55.569290Z","iopub.status.idle":"2024-04-15T16:12:55.575629Z","shell.execute_reply.started":"2024-04-15T16:12:55.569252Z","shell.execute_reply":"2024-04-15T16:12:55.574575Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Evaluation Type\nCoherence      0.825\nConsistency    0.900\nFluency        0.900\nRelevance      0.850\ndtype: float64\n","output_type":"stream"}]}]}