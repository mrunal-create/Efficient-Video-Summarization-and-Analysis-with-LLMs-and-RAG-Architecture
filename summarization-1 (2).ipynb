{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment setup","metadata":{"id":"EpGWUswStQEZ"}},{"cell_type":"code","source":"# !pip install -q -U transformers peft accelerate optimum\n# !pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\n# !pip install langchain\n# !pip install einops\n# !pip install optimum\n# !pip install rouge_score\n# !pip install bert_score\n# !pip install pytube ffmpeg-python --quiet\n# !pip install SpeechRecognition --quiet\n# !pip install pydub --quiet\n# !pip install moviepy --quiet\n# !pip install openai --quiet","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJ57vxf5ih2v","outputId":"92d77c3f-23d3-4de1-eba5-c388d9ffb6d9","execution":{"iopub.status.busy":"2024-04-14T17:46:12.947284Z","iopub.execute_input":"2024-04-14T17:46:12.948165Z","iopub.status.idle":"2024-04-14T17:46:12.952927Z","shell.execute_reply.started":"2024-04-14T17:46:12.948130Z","shell.execute_reply":"2024-04-14T17:46:12.951759Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:47:34.400931Z","iopub.execute_input":"2024-04-14T18:47:34.402186Z","iopub.status.idle":"2024-04-14T18:47:48.114428Z","shell.execute_reply.started":"2024-04-14T18:47:34.402153Z","shell.execute_reply":"2024-04-14T18:47:48.113161Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"markdown","source":"# Environment settings","metadata":{"id":"vflg3lYz1Bws"}},{"cell_type":"code","source":"# from openai import OpenAI\n\n# client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"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\"))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:50:14.780640Z","iopub.execute_input":"2024-04-14T18:50:14.781521Z","iopub.status.idle":"2024-04-14T18:50:15.280209Z","shell.execute_reply.started":"2024-04-14T18:50:14.781481Z","shell.execute_reply":"2024-04-14T18:50:15.279239Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"markdown","source":"## Import neccessary libraries","metadata":{"id":"XGf07gk3tV7b"}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport math\n# import evaluate\nfrom sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom dotenv import load_dotenv\nfrom pytube import YouTube\nimport ffmpeg\nimport requests\nfrom urllib.parse import urlparse\nimport requests\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport speech_recognition as sr\nfrom pydub import AudioSegment\nfrom pydub.utils import make_chunks\nimport os\n# load_dotenv() ##load all the nevironment variables\nimport os\nimport google.generativeai as genai\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig\nfrom pytube import YouTube\nimport ffmpeg\nimport requests\nfrom urllib.parse import urlparse\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom moviepy.editor import *\nimport asyncio\nimport tarfile\nfrom zipfile import ZipFile\nfrom langchain import LLMChain, HuggingFacePipeline, PromptTemplate","metadata":{"id":"BtqOIHeHtQbY","execution":{"iopub.status.busy":"2024-04-14T17:46:12.980348Z","iopub.execute_input":"2024-04-14T17:46:12.980672Z","iopub.status.idle":"2024-04-14T17:46:12.993901Z","shell.execute_reply.started":"2024-04-14T17:46:12.980643Z","shell.execute_reply":"2024-04-14T17:46:12.993067Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{"id":"XeGe4gdg08bs"}},{"cell_type":"code","source":"#Just update model_name here and test out your model , also update id of model in code below cell 12\nconfig={\n    \"model_id\": [\"TheBloke/Llama-2-7b-Chat-GPTQ\",\n                 \"TheBloke/Llama-2-7B-AWQ\",\n                 \"TheBloke/Llama-2-7B-GGUF\",\n                 \"TheBloke/Llama-2-7B-GGML\",\n                 \"TheBloke/Llama-2-7B-fp16\",\n                 \"TheBloke/Llama-2-7B-GPTQ\",\n                 \"TheBloke/llama-2-7B-Guanaco-QLoRA-AWQ\",\n                 \"TheBloke/Llama-2-7B-AWQ\",\n                \"google/flan-t5-large\"],\n    \"hf_token\": \"...\",\n    \"model\": {\n        \"temperature\": 0.7, # [0, 0.7, .0.9, 1.1, 1.3]  Testing iteratively.\n        \"max_length\": 4000,\n        \"top_k\": 10,\n        \"num_return\": 1\n    },\n    \"dataset\": [\n        \"billsum\",\n        \"cnn_dailymail\",\n        \"big_patent\"\n    ],\n    \"eval\": ['rouge', \"bertscore\", 'meteor']\n}\n","metadata":{"id":"Ilgcn7sW06Qu","execution":{"iopub.status.busy":"2024-04-14T17:46:12.995775Z","iopub.execute_input":"2024-04-14T17:46:12.996071Z","iopub.status.idle":"2024-04-14T17:46:13.009951Z","shell.execute_reply.started":"2024-04-14T17:46:12.996038Z","shell.execute_reply":"2024-04-14T17:46:13.009207Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"id":"4B0V3PgStZih"}},{"cell_type":"code","source":"def call_parameter(model):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    untrainable_params = pytorch_total_params - trainable_params\n    print(f'Model {model.__class__.__name__} has {pytorch_total_params} parameters in total\\n'\\\n        f'Trainable parameters: {trainable_params}\\nUntrainable parameters: {untrainable_params}')\n    return pytorch_total_params","metadata":{"id":"mIvth5iatQeJ","execution":{"iopub.status.busy":"2024-04-14T17:46:13.010955Z","iopub.execute_input":"2024-04-14T17:46:13.011292Z","iopub.status.idle":"2024-04-14T17:46:13.024281Z","shell.execute_reply.started":"2024-04-14T17:46:13.011261Z","shell.execute_reply":"2024-04-14T17:46:13.023514Z"},"trusted":true},"execution_count":195,"outputs":[]},{"cell_type":"markdown","source":"# Model architecture","metadata":{"id":"CxmLQrHytd-m"}},{"cell_type":"code","source":"def generate_model(model_id, config):\n    print(f\"Setting up model {model_id}\")\n    model = AutoModelForCausalLM.from_pretrained(model_id, use_safetensors=True,\n                            device_map='auto', trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(model_id,\n                                            device_map='auto', trust_remote_code=True)\n    return model, tokenizer","metadata":{"id":"JxZI5LT83upi","execution":{"iopub.status.busy":"2024-04-14T17:46:13.025475Z","iopub.execute_input":"2024-04-14T17:46:13.025800Z","iopub.status.idle":"2024-04-14T17:46:13.038318Z","shell.execute_reply.started":"2024-04-14T17:46:13.025771Z","shell.execute_reply":"2024-04-14T17:46:13.037545Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"class Agent:\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def __repr__(self):\n        return f'Model {self.model.__class__.__name__}'","metadata":{"id":"jCresULjzlcJ","execution":{"iopub.status.busy":"2024-04-14T17:46:13.039216Z","iopub.execute_input":"2024-04-14T17:46:13.039469Z","iopub.status.idle":"2024-04-14T17:46:13.048994Z","shell.execute_reply.started":"2024-04-14T17:46:13.039447Z","shell.execute_reply":"2024-04-14T17:46:13.048143Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"#Update the model_id index as per model you want to use\nmodel, tokenizer = generate_model(config['model_id'][0], config)\nno_params = call_parameter(model)\nprint(\"=====\"*5)\nprint(f\"Model {config['model_id'][0]} has {no_params} parameters.\")","metadata":{"id":"DYmsnNkXjCRB","execution":{"iopub.status.busy":"2024-04-14T17:46:13.062260Z","iopub.execute_input":"2024-04-14T17:46:13.062543Z","iopub.status.idle":"2024-04-14T17:46:17.380291Z","shell.execute_reply.started":"2024-04-14T17:46:13.062521Z","shell.execute_reply":"2024-04-14T17:46:17.379306Z"},"trusted":true},"execution_count":199,"outputs":[{"name":"stdout","text":"Setting up model TheBloke/Llama-2-7b-Chat-GPTQ\nModel LlamaForCausalLM has 262410240 parameters in total\nTrainable parameters: 262410240\nUntrainable parameters: 0\n=========================\nModel TheBloke/Llama-2-7b-Chat-GPTQ has 262410240 parameters.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Experiments","metadata":{"id":"x506HiO4thgt"}},{"cell_type":"markdown","source":"## Implementation","metadata":{"id":"85ujx14hyZbR"}},{"cell_type":"code","source":"class Generator:\n    def __init__(self, config, agent, template):\n        self.agent = agent\n        pipeline = transformers.pipeline(\n            \"text-generation\",\n            model=self.agent.model,\n            tokenizer=self.agent.tokenizer,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n            device_map=\"auto\",\n            max_length=config['model']['max_length'],\n            do_sample=True,\n            top_k=config['model']['top_k'],\n            num_return_sequences=config['model']['num_return'],\n            pad_token_id=tokenizer.eos_token_id\n        )\n        llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': config['model']['temperature']})\n        prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n        self.llm_chain = LLMChain(prompt=prompt, llm=llm)\n\n    def generate(self, text):\n        result = self.llm_chain.invoke(text)\n        return result","metadata":{"id":"Akq4pDpW52k4","execution":{"iopub.status.busy":"2024-04-14T17:46:17.383319Z","iopub.execute_input":"2024-04-14T17:46:17.383653Z","iopub.status.idle":"2024-04-14T17:46:17.391372Z","shell.execute_reply.started":"2024-04-14T17:46:17.383626Z","shell.execute_reply":"2024-04-14T17:46:17.390385Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"\n              Write a summary of the following text delimited by triple backticks.\n              Return your response which covers the key points of the text.\n              ```{text}```\n              SUMMARY:\n           \"\"\"","metadata":{"id":"dQsvQqZ-tQpH","execution":{"iopub.status.busy":"2024-04-14T17:46:17.392461Z","iopub.execute_input":"2024-04-14T17:46:17.392711Z","iopub.status.idle":"2024-04-14T17:46:17.409239Z","shell.execute_reply.started":"2024-04-14T17:46:17.392690Z","shell.execute_reply":"2024-04-14T17:46:17.408371Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"agent = Agent(model, tokenizer)\nllm_agent = Generator(config, agent, template)","metadata":{"id":"IsSok7Am0--B","execution":{"iopub.status.busy":"2024-04-14T17:46:17.410310Z","iopub.execute_input":"2024-04-14T17:46:17.410625Z","iopub.status.idle":"2024-04-14T17:46:17.427509Z","shell.execute_reply.started":"2024-04-14T17:46:17.410579Z","shell.execute_reply":"2024-04-14T17:46:17.426554Z"},"trusted":true},"execution_count":202,"outputs":[]},{"cell_type":"markdown","source":"Youtube links I tested on:\n\n* \"https://www.youtube.com/watch?v=U1omz0B9FTw\"\n \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\n* https://www.youtube.com/watch?v=h5id4erwD4s\n* https://www.youtube.com/watch?v=tl30y5OOfqQ,\n* https://www.youtube.com/watch?v=6bJTEZnTT5A\n* https://www.youtube.com/watch?v=SZorAJ4I-sA\n* https://www.youtube.com/watch?v=V_2QqOEwzYU\n* https://www.youtube.com/watch?v=yw-E__nDkKU\n* https://www.youtube.com/watch?v=en2bmeB4QUo\n* https://www.youtube.com/watch?v=ch5EQgzfroo","metadata":{}},{"cell_type":"markdown","source":"# **SUMMARIZATION PART**","metadata":{}},{"cell_type":"code","source":"#final = dict()\n#intialize dictionary to store your transcript and summaries generated \n#run only once in start","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_mp4_playable(file_path):\n  try:\n      # Probe the file to get information about it\n      probe = ffmpeg.probe(file_path)\n      # Check if the file format is recognized as video\n      if 'streams' in probe and any(stream['codec_type'] == 'video' for stream in probe['streams']):\n          print(\"MP4 file is playable.\")\n          return True\n      else:\n          print(\"MP4 file is corrupt or non-playable.\")\n          return False\n  except ffmpeg.Error as e:\n      #print(\"Error occurred:\", e.stderr)\n      return False\n# Extract audio from given video\ndef extract_audio(link, output_file_path):\n  if urlparse(link).netloc == \"www.youtube.com\":\n    yt = YouTube(link)\n    video_path = yt.streams[0].url\n\n  elif urlparse(link).netloc == \"www.linkedin.com\":\n    r = requests.get(link)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    data_linkedin = json.loads(soup.find('script', type='application/ld+json').text)\n    if data_linkedin['isAccessibleForFree'] == True:\n      video_path = json.loads(soup.video['data-sources'])[0]['src']\n    else:\n      print(\"Sorry! Can't extract audio. Please make sure the video is free for access.\")\n      return\n\n  elif urlparse(link).netloc == \"www.coursera.org\":\n    r = requests.get(link)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    data_coursera = json.loads(soup.find('script', type='application/ld+json').text)\n    video_path = data_coursera['@graph'][1]['contentURL']\n\n  elif urlparse(link).netloc not in [\"www.youtube.com\", \"www.linkedin.com\", \"www.coursera.org\"]:\n    print(\"Sorry! Can't extract audio. Please make sure the video link is valid.\")\n    return\n\n\n  else:\n    if(is_mp4_playable(link)):\n      video_path = link\n    else:\n      print(\"Sorry! Can't extract audio. Please make sure the video file exists and is not corrupted.\")\n      return\n\n\n  audio, err = (\n      ffmpeg\n      .input(video_path)\n      .output(\"pipe:\", format='mp3', acodec='libmp3lame', audio_bitrate='320k')\n      .run(capture_stdout=True)\n  )\n\n\n  with open(output_file_path, 'wb') as f:\n      f.write(audio)\n\n  print(\"Audio extraction complete.\")\ndef divide_audio_chunks(audio_file, chunksize = 30000):\n    mp3_audio = AudioSegment.from_mp3(audio_file)\n    # Split the audio into chunks\n    chunks = make_chunks(mp3_audio, chunksize)\n    return chunks\n\ndef process_chunks(audio_chunks):\n  whole_speech = \"\"\n  recognizer = sr.Recognizer()\n  for i, chunk in enumerate(audio_chunks):\n    audio = chunk.export(format=\"wav\")\n    with sr.AudioFile(audio) as source:\n      audio_data = recognizer.record(source)\n      try:\n        text = recognizer.recognize_google(audio_data)\n        #print(f\"Chunk {i+1}: {text}\")\n        whole_speech += text\n      except sr.UnknownValueError:\n        print(f\"Chunk {i+1}, Google Speech Recognition could not understand audio\")\n      except sr.RequestError as e:\n        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n  return whole_speech\n## getting the transcript data from yt videos\ndef extract_audio_transcript_details(youtube_video_url):\n\n    try:\n        sound_file='audio.mp3'\n        extract_audio(youtube_video_url,sound_file)\n        #video_id=youtube_video_url.split(\"=\")[1]\n        audio_chunks = divide_audio_chunks(sound_file)\n        speech = process_chunks(audio_chunks)\n        return speech\n\n    except Exception as e:\n        raise e\n    \nyoutube_link = \"https://www.youtube.com/watch?v=ch5EQgzfroo\"\nif youtube_link:\n    video_id = youtube_link.split(\"=\")[1]\n    transcript_text=extract_audio_transcript_details(youtube_link)\n    if transcript_text:\n        generated_sample = llm_agent.generate(transcript_text)\n        print(\"SUMMARY\")\n        text=generated_sample[\"text\"].split(\"SUMMARY\")[1]\n        print(text)\n        final[transcript_text]=generated_sample[\"text\"].split(\"SUMMARY\")[1]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:46:17.430756Z","iopub.execute_input":"2024-04-14T17:46:17.431048Z","iopub.status.idle":"2024-04-14T17:53:20.929433Z","shell.execute_reply.started":"2024-04-14T17:46:17.431013Z","shell.execute_reply":"2024-04-14T17:53:20.928406Z"},"trusted":true},"execution_count":203,"outputs":[{"name":"stderr","text":"ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libavresample   4.  0.  0 /  4.  0.  0\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from 'https://rr4---sn-qxoedn7k.googlevideo.com/videoplayback?expire=1713138378&ei=ahYcZuTIFqymlu8P8r6XOA&ip=34.173.202.63&id=o-ABHc7BLrSMZemPIGpLCaBZ6sMbAv_hynNxfxntLPZA05&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=-K&mm=31%2C29&mn=sn-qxoedn7k%2Csn-qxo7rn7r&ms=au%2Crdu&mv=m&mvi=4&pl=17&initcwndbps=8598750&bui=AaUN6a2jSleE8AG16nk6AQ1yePr4ham-Hbm6PI8KxoYyop1J6uIeKvviCIPArNfKCQIROenunmMCvdkX&vprv=1&mime=video%2Fmp4&gir=yes&clen=36531513&ratebypass=yes&dur=636.621&lmt=1670960659854056&mt=1713116449&fvip=5&c=ANDROID_EMBEDDED_PLAYER&txp=5538434&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Cmime%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRAIgAabj_uQsXF82lN8H10lbZYQGvTO0bIVUcy19mYNuMsUCIGg8BZLqzQRWRmZpevclcxWx4wcIqgVYu2iYONV27aRZ&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=ALClDIEwRgIhAOpbW7qsy3HvY87_dyJ5QIUfI34mXJWENpS3AA2VcODZAiEAvVXPhB6Wp-9IVFpoQAW7TMESns7QhLLjZvyaS_hHWaY%3D':\n  Metadata:\n    major_brand     : mp42\n    minor_version   : 0\n    compatible_brands: isommp42\n    encoder         : Google\n  Duration: 00:10:36.62, start: 0.000000, bitrate: 459 kb/s\n    Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 360 kb/s, 23.98 fps, 23.98 tbr, 24k tbn, 47.95 tbc (default)\n    Metadata:\n      handler_name    : ISO Media file produced by Google Inc.\n    Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n    Metadata:\n      handler_name    : ISO Media file produced by Google Inc.\nStream mapping:\n  Stream #0:1 -> #0:0 (aac (native) -> mp3 (libmp3lame))\nPress [q] to stop, [?] for help\nOutput #0, mp3, to 'pipe:':\n  Metadata:\n    major_brand     : mp42\n    minor_version   : 0\n    compatible_brands: isommp42\n    TSSE            : Lavf58.29.100\n    Stream #0:0(eng): Audio: mp3 (libmp3lame), 44100 Hz, stereo, fltp, 320 kb/s (default)\n    Metadata:\n      handler_name    : ISO Media file produced by Google Inc.\n      encoder         : Lavc58.54.100 libmp3lame\nsize=   24870kB time=00:10:36.63 bitrate= 320.0kbits/s speed=44.5x    \nvideo:0kB audio:24869kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000542%\n","output_type":"stream"},{"name":"stdout","text":"Audio extraction complete.\nChunk 22, Google Speech Recognition could not understand audio\nSUMMARY\n:\n            The text discusses James Cameron's new movie, Avatar 2, and what to expect from the film. The author highlights that the movie will feature a simple but engaging storyline, groundbreaking technology, and an inspiring message on protecting the environment. The author watched the movie and found it visually stunning. The film's first half did not hold their attention, but the second half was thoroughly engrossing and exciting. The writer praises Cameron for utilizing his mastery of filmmaking to create an immersive experience in 3D, with attention to detail and invention to improve the viewing experience. They also discuss the character of Spider, who is a young human adopted by the Navi and whose motivations are unclear to the author. They encourage viewers to see the movie in theaters to appreciate Cameron's originality and environmental message. Finally, the author highlights the film's potential to inspire environmental activism and the importance of protecting the planet. \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **SUMMARY OF VIDEO**","metadata":{}},{"cell_type":"code","source":"generated_sample[\"text\"].split(\"SUMMARY\")[1]","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:53:20.930993Z","iopub.execute_input":"2024-04-14T17:53:20.931286Z","iopub.status.idle":"2024-04-14T17:53:20.937104Z","shell.execute_reply.started":"2024-04-14T17:53:20.931261Z","shell.execute_reply":"2024-04-14T17:53:20.936177Z"},"trusted":true},"execution_count":204,"outputs":[{"execution_count":204,"output_type":"execute_result","data":{"text/plain":"\":\\n            The text discusses James Cameron's new movie, Avatar 2, and what to expect from the film. The author highlights that the movie will feature a simple but engaging storyline, groundbreaking technology, and an inspiring message on protecting the environment. The author watched the movie and found it visually stunning. The film's first half did not hold their attention, but the second half was thoroughly engrossing and exciting. The writer praises Cameron for utilizing his mastery of filmmaking to create an immersive experience in 3D, with attention to detail and invention to improve the viewing experience. They also discuss the character of Spider, who is a young human adopted by the Navi and whose motivations are unclear to the author. They encourage viewers to see the movie in theaters to appreciate Cameron's originality and environmental message. Finally, the author highlights the film's potential to inspire environmental activism and the importance of protecting the planet. \""},"metadata":{}}]},{"cell_type":"code","source":"print('length of dictionary with summaries',len(final))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:53:20.965350Z","iopub.execute_input":"2024-04-14T17:53:20.966216Z","iopub.status.idle":"2024-04-14T17:53:20.978276Z","shell.execute_reply.started":"2024-04-14T17:53:20.966192Z","shell.execute_reply":"2024-04-14T17:53:20.977304Z"},"trusted":true},"execution_count":207,"outputs":[{"execution_count":207,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"code","source":"input_text=[]\nsummaries=[]\nfor key,value in final.items():\n    input_text.append(key)\n    summaries.append(value)  \n# Path where you want to save the CSV file which contains text and summaries\nfile_path = 'output.csv'\nwith open(file_path, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['input_text', 'summaries'])\n    # Write data rows\n    for a, b in zip(input_text, summaries):\n        writer.writerow([a, b])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:28:12.515284Z","iopub.execute_input":"2024-04-14T18:28:12.515698Z","iopub.status.idle":"2024-04-14T18:28:12.526623Z","shell.execute_reply.started":"2024-04-14T18:28:12.515666Z","shell.execute_reply":"2024-04-14T18:28:12.525768Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"markdown","source":"# EVALUATION OF GENERATED SUMMARIES USING G-EVAL ","metadata":{}},{"cell_type":"markdown","source":"Here we implement an example reference-free text evaluator using gpt-4, inspired by the G-Eval) framework which evaluates the quality of generated text using large language models. Unlike metrics like ROUGE or BERTScore that rely on comparison to reference summaries, the gpt-4 based evaluator assesses the quality of generated content based solely on the input prompt and text, without any ground truth references. This makes it applicable to new datasets and tasks where human references are sparse or unavailable.\n\nHere's an overview of this method:\n\nWe define four distinct criteria:\nRelevance: Evaluates if the summary includes only important information and excludes redundancies.\nCoherence: Assesses the logical flow and organization of the summary.\nConsistency: Checks if the summary aligns with the facts in the source document.\nFluency: Rates the grammar and readability of the summary.\nWe craft prompts for each of these criteria, taking the original document and the summary as inputs, and leveraging chain-of-thought generation and guiding the model to output a numeric score from 1-5 for each criteria.\nWe generate scores from gpt-4 with the defined prompts, comparing them across summaries.\nIn this demonstration, we're using a direct scoring function where gpt-4 generates a discrete score (1-5) for each metric. Normalizing the scores and taking a weighted sum could result in more robust, continuous scores that better reflect the quality and diversity of the summaries.","metadata":{}}]}